{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and process data\n",
    "path = r'odi_csv/'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "shuffle(all_files)\n",
    "innings2 = []\n",
    "targ_score = []\n",
    "target=[]\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename,usecols=[0])\n",
    "    skips = df.loc[ 'info' , : ].shape[0]\n",
    "\n",
    "    df = pd.read_csv(filename,nrows=skips,skiprows=1,header=None)\n",
    "    df = df.drop(columns=0).set_index(df.columns[1])\n",
    "    winteam=None\n",
    "    if 'winner' in df.index:\n",
    "        winteam = df.loc['winner',:].values[0]\n",
    "    \n",
    "    df = pd.read_csv(filename,skiprows=skips+1,header=None)\n",
    "    df2 = df[df.columns[[1,2,7,8,9]]].set_index(df.columns[1]).drop(index=1)\n",
    "    df2[9] = df2[9].notna() * 1\n",
    "    df1_ = df[df.columns[[1,2,7,8]]].set_index(df.columns[1])\n",
    "    df1 = df1_[df1_.index==1]\n",
    "\n",
    "    if df2.shape[0]>0:\n",
    "        innings2.append(df2)\n",
    "        targ_score.append(df1[df1.columns[[1,2]]].sum().sum())\n",
    "        i2team = df[df.columns[[1,3]]].set_index(df.columns[1]).drop(index=1).values[0,0]\n",
    "        if (i2team==winteam):\n",
    "            target.append(1)\n",
    "        else:\n",
    "            target.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1244, 321, 4])\n",
      "torch.Size([312, 321, 4])\n"
     ]
    }
   ],
   "source": [
    "# torch tensor processing\n",
    "features=[]\n",
    "# create targets first\n",
    "for i in range(len(innings2)):\n",
    "    features.append(torch.tensor(innings2[i].values))\n",
    "\n",
    "\n",
    "# convert to fixed length sequence\n",
    "features = pad_sequence(features,batch_first=True, padding_value=-1)\n",
    "targets = torch.tensor(target)\n",
    "\n",
    "split = int(len(features) * 0.8)\n",
    "X_train = features[:split]\n",
    "X_test  = features[split:]\n",
    "y_train = targets[:split]\n",
    "y_test  = targets[split:]\n",
    "print(X_train.size())\n",
    "print(X_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1244, 321, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_score_train = np.transpose(targ_score[:split] * np.ones((1,X_train.shape[1],X_train.shape[0])))\n",
    "targ_score_test = np.transpose(targ_score[split:] * np.ones((1,X_test.shape[1],X_test.shape[0])))\n",
    "X_train = torch.cat((X_train.float(), torch.Tensor(targ_score_train)), 2)\n",
    "X_test = torch.cat((X_test.float(), torch.Tensor(targ_score_test)), 2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "class matchRNN(nn.Module):\n",
    "    def __init__(self,insize,hsize,outsize):\n",
    "        super(matchRNN,self).__init__()\n",
    "        \n",
    "        self.insize=insize\n",
    "        self.hsize=hsize\n",
    "        self.outsize = outsize\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=insize, hidden_size=hsize)\n",
    "        self.fc_out = nn.Linear(in_features=hsize, out_features=outsize)\n",
    "#         self.dropout = nn.Dropout(p=0.2, inplace=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,feat):\n",
    "#         feat = torch.tensor(feat[np.newaxis,:],dtype=torch.float32)\n",
    "        batch_size = feat.size(0)\n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hsize))\n",
    "        cell_state = torch.zeros((batch_size, self.hsize))\n",
    "        outputs = torch.empty((batch_size, feat.size(1), self.outsize))\n",
    "\n",
    "        for t in range(feat.size(1)):\n",
    "\n",
    "            # for the first time step (if input is different)\n",
    "            if t == 0:\n",
    "                hidden_state, cell_state = self.lstm_cell(feat[:,t,:].view(batch_size,-1).float(), (hidden_state, cell_state))\n",
    "                \n",
    "            # for the 2nd+ time step\n",
    "            else:\n",
    "                hidden_state, cell_state = self.lstm_cell(feat[:,t,:].view(batch_size,-1).float(), (hidden_state, cell_state))\n",
    "            \n",
    "#             dropouts = self.dropout(hidden_state)\n",
    "            out = self.fc_out(hidden_state)\n",
    "#             out = self.softmax(out)\n",
    "            outputs[:,t,:] = out\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1244, 321])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain_tiled = y_train.repeat(X_train.size(1),1).transpose(0,1)\n",
    "ytrain_tiled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalfunc(model,j):\n",
    "    model.eval()\n",
    "    # train\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    corr=0\n",
    "    num_batches = X.size(0) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        op = model(X[start_idx:end_idx])\n",
    "        req_op = op[:,j-1]\n",
    "        maxval,maxidx = torch.max(req_op,1)\n",
    "        corr+= np.sum((maxidx==y[start_idx:end_idx]).numpy())\n",
    "    total=num_batches*batch_size\n",
    "    train_acc = corr / total\n",
    "    \n",
    "    # test\n",
    "    X = X_test\n",
    "    y = y_test\n",
    "    corr=0\n",
    "    num_batches = X.size(0) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        op = model(X[start_idx:end_idx])\n",
    "        req_op = op[:,j-1]\n",
    "        maxval,maxidx = torch.max(req_op,1)\n",
    "        corr+= np.sum((maxidx==y[start_idx:end_idx]).numpy())\n",
    "    total=num_batches*batch_size\n",
    "    test_acc = corr / total\n",
    "    print('j = {}, Train Acc = {}, Test Acc = {}'.format(j,train_acc,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, loss = 48.45524287223816\n",
      "j = 321, Train Acc = 0.7037337662337663, Test Acc = 0.7401315789473685\n",
      "j = 250, Train Acc = 0.7077922077922078, Test Acc = 0.7171052631578947\n",
      "j = 200, Train Acc = 0.7224025974025974, Test Acc = 0.75\n",
      "Epoch = 1, loss = 44.65857154130936\n",
      "Epoch = 2, loss = 43.73495411872864\n",
      "Epoch = 3, loss = 43.30782839655876\n",
      "Epoch = 4, loss = 42.82588878273964\n",
      "Epoch = 5, loss = 42.524042904376984\n",
      "Epoch = 6, loss = 42.30747798085213\n",
      "Epoch = 7, loss = 42.16320013999939\n",
      "Epoch = 8, loss = 41.93680381774902\n",
      "Epoch = 9, loss = 41.71491289138794\n",
      "Epoch = 10, loss = 41.60006842017174\n",
      "j = 321, Train Acc = 0.7402597402597403, Test Acc = 0.7664473684210527\n",
      "j = 250, Train Acc = 0.7451298701298701, Test Acc = 0.7828947368421053\n",
      "j = 200, Train Acc = 0.737012987012987, Test Acc = 0.7796052631578947\n",
      "Epoch = 11, loss = 41.4481817483902\n",
      "Epoch = 12, loss = 41.28880748152733\n",
      "Epoch = 13, loss = 41.156962126493454\n",
      "Epoch = 14, loss = 41.03136560320854\n",
      "Epoch = 15, loss = 40.90825682878494\n",
      "Epoch = 16, loss = 40.76008287072182\n",
      "Epoch = 17, loss = 40.63943353295326\n",
      "Epoch = 18, loss = 40.46027198433876\n",
      "Epoch = 19, loss = 40.431445240974426\n",
      "Epoch = 20, loss = 40.282871305942535\n",
      "j = 321, Train Acc = 0.7491883116883117, Test Acc = 0.7763157894736842\n",
      "j = 250, Train Acc = 0.7662337662337663, Test Acc = 0.7894736842105263\n",
      "j = 200, Train Acc = 0.7451298701298701, Test Acc = 0.7763157894736842\n",
      "Epoch = 21, loss = 40.09994012117386\n",
      "Epoch = 22, loss = 39.924460619688034\n",
      "Epoch = 23, loss = 39.81862795352936\n",
      "Epoch = 24, loss = 39.65613675117493\n",
      "Epoch = 25, loss = 39.51662638783455\n",
      "Epoch = 26, loss = 39.543946236371994\n",
      "Epoch = 27, loss = 39.318697422742844\n",
      "Epoch = 28, loss = 39.183277279138565\n",
      "Epoch = 29, loss = 39.38351237773895\n",
      "Epoch = 30, loss = 38.96764487028122\n",
      "j = 321, Train Acc = 0.7662337662337663, Test Acc = 0.7730263157894737\n",
      "j = 250, Train Acc = 0.786525974025974, Test Acc = 0.7993421052631579\n",
      "j = 200, Train Acc = 0.7775974025974026, Test Acc = 0.7927631578947368\n",
      "Epoch = 31, loss = 38.786161452531815\n",
      "Epoch = 32, loss = 38.64976581931114\n",
      "Epoch = 33, loss = 38.44884794950485\n",
      "Epoch = 34, loss = 38.31016370654106\n",
      "Epoch = 35, loss = 38.050492614507675\n",
      "Epoch = 36, loss = 37.6956784427166\n",
      "Epoch = 37, loss = 37.64049819111824\n",
      "Epoch = 38, loss = 36.8508023917675\n",
      "Epoch = 39, loss = 36.21753281354904\n",
      "Epoch = 40, loss = 35.8295835852623\n",
      "j = 321, Train Acc = 0.788961038961039, Test Acc = 0.7828947368421053\n",
      "j = 250, Train Acc = 0.797077922077922, Test Acc = 0.8256578947368421\n",
      "j = 200, Train Acc = 0.7881493506493507, Test Acc = 0.819078947368421\n",
      "Epoch = 41, loss = 37.42051577568054\n",
      "Epoch = 42, loss = 36.77998673915863\n",
      "Epoch = 43, loss = 35.187932908535004\n",
      "Epoch = 44, loss = 35.12283447384834\n",
      "Epoch = 45, loss = 35.33588618040085\n",
      "Epoch = 46, loss = 35.41923913359642\n",
      "Epoch = 47, loss = 34.617898374795914\n",
      "Epoch = 48, loss = 35.56024184823036\n",
      "Epoch = 49, loss = 34.92148803174496\n",
      "Epoch = 50, loss = 34.62491989135742\n",
      "j = 321, Train Acc = 0.875, Test Acc = 0.8881578947368421\n",
      "j = 250, Train Acc = 0.8198051948051948, Test Acc = 0.8355263157894737\n",
      "j = 200, Train Acc = 0.7962662337662337, Test Acc = 0.8026315789473685\n",
      "Epoch = 51, loss = 32.85739503800869\n",
      "Epoch = 52, loss = 32.47512100636959\n",
      "Epoch = 53, loss = 32.09735542535782\n",
      "Epoch = 54, loss = 31.450697749853134\n",
      "Epoch = 55, loss = 30.95978757739067\n",
      "Epoch = 56, loss = 30.246582373976707\n",
      "Epoch = 57, loss = 29.53728859126568\n",
      "Epoch = 58, loss = 29.5852490067482\n",
      "Epoch = 59, loss = 28.805572614073753\n",
      "Epoch = 60, loss = 28.147159591317177\n",
      "j = 321, Train Acc = 0.9261363636363636, Test Acc = 0.9243421052631579\n",
      "j = 250, Train Acc = 0.8766233766233766, Test Acc = 0.875\n",
      "j = 200, Train Acc = 0.8538961038961039, Test Acc = 0.8618421052631579\n",
      "Epoch = 61, loss = 27.936473667621613\n",
      "Epoch = 62, loss = 27.302377700805664\n",
      "Epoch = 63, loss = 27.171275451779366\n",
      "Epoch = 64, loss = 26.635015696287155\n",
      "Epoch = 65, loss = 26.651667669415474\n",
      "Epoch = 66, loss = 26.194896891713142\n",
      "Epoch = 67, loss = 26.11676074564457\n",
      "Epoch = 68, loss = 28.173260763287544\n",
      "Epoch = 69, loss = 26.841281414031982\n",
      "Epoch = 70, loss = 26.623045846819878\n",
      "j = 321, Train Acc = 0.9569805194805194, Test Acc = 0.944078947368421\n",
      "j = 250, Train Acc = 0.8790584415584416, Test Acc = 0.8947368421052632\n",
      "j = 200, Train Acc = 0.8587662337662337, Test Acc = 0.875\n",
      "Epoch = 71, loss = 26.5699422955513\n",
      "Epoch = 72, loss = 26.29414175450802\n",
      "Epoch = 73, loss = 26.328852593898773\n",
      "Epoch = 74, loss = 26.24229606986046\n",
      "Epoch = 75, loss = 26.13706086575985\n",
      "Epoch = 76, loss = 25.983620777726173\n",
      "Epoch = 77, loss = 25.841575160622597\n",
      "Epoch = 78, loss = 25.706169068813324\n",
      "Epoch = 79, loss = 25.59032654762268\n",
      "Epoch = 80, loss = 25.441688403487206\n",
      "j = 321, Train Acc = 0.9659090909090909, Test Acc = 0.9572368421052632\n",
      "j = 250, Train Acc = 0.9058441558441559, Test Acc = 0.9111842105263158\n",
      "j = 200, Train Acc = 0.8847402597402597, Test Acc = 0.8947368421052632\n",
      "Epoch = 81, loss = 25.26094265282154\n",
      "Epoch = 82, loss = 25.098144188523293\n",
      "Epoch = 83, loss = 25.022753566503525\n",
      "Epoch = 84, loss = 24.9406920671463\n",
      "Epoch = 85, loss = 24.823637202382088\n",
      "Epoch = 86, loss = 24.714464649558067\n",
      "Epoch = 87, loss = 24.748789370059967\n",
      "Epoch = 88, loss = 24.550839588046074\n",
      "Epoch = 89, loss = 24.527492865920067\n",
      "Epoch = 90, loss = 24.462069526314735\n",
      "j = 321, Train Acc = 0.9699675324675324, Test Acc = 0.9506578947368421\n",
      "j = 250, Train Acc = 0.9066558441558441, Test Acc = 0.9210526315789473\n",
      "j = 200, Train Acc = 0.885551948051948, Test Acc = 0.8848684210526315\n",
      "Epoch = 91, loss = 24.3830783367157\n",
      "Epoch = 92, loss = 24.372505486011505\n",
      "Epoch = 93, loss = 24.313007727265358\n",
      "Epoch = 94, loss = 24.250984728336334\n",
      "Epoch = 95, loss = 24.170839235186577\n",
      "Epoch = 96, loss = 24.151909202337265\n",
      "Epoch = 97, loss = 24.122692108154297\n",
      "Epoch = 98, loss = 24.05288377404213\n",
      "Epoch = 99, loss = 24.03533335030079\n",
      "Epoch = 100, loss = 24.050723299384117\n",
      "j = 321, Train Acc = 0.9667207792207793, Test Acc = 0.9539473684210527\n",
      "j = 250, Train Acc = 0.9058441558441559, Test Acc = 0.9243421052631579\n",
      "j = 200, Train Acc = 0.8806818181818182, Test Acc = 0.881578947368421\n",
      "Epoch = 101, loss = 23.914033472537994\n",
      "Epoch = 102, loss = 24.17840440571308\n",
      "Epoch = 103, loss = 23.80204789340496\n",
      "Epoch = 104, loss = 23.843407303094864\n",
      "Epoch = 105, loss = 23.904913619160652\n",
      "Epoch = 106, loss = 23.790258273482323\n",
      "Epoch = 107, loss = 23.879646718502045\n",
      "Epoch = 108, loss = 23.789689511060715\n",
      "Epoch = 109, loss = 23.88639362156391\n",
      "Epoch = 110, loss = 23.740693971514702\n",
      "j = 321, Train Acc = 0.9659090909090909, Test Acc = 0.9638157894736842\n",
      "j = 250, Train Acc = 0.9082792207792207, Test Acc = 0.9177631578947368\n",
      "j = 200, Train Acc = 0.8814935064935064, Test Acc = 0.8717105263157895\n",
      "Epoch = 111, loss = 23.69471548497677\n",
      "Epoch = 112, loss = 23.76016838848591\n",
      "Epoch = 113, loss = 23.704495146870613\n",
      "Epoch = 114, loss = 23.801116824150085\n",
      "Epoch = 115, loss = 23.558096691966057\n",
      "Epoch = 116, loss = 23.640105694532394\n",
      "Epoch = 117, loss = 23.583515837788582\n",
      "Epoch = 118, loss = 23.70819279551506\n",
      "Epoch = 119, loss = 23.334608018398285\n",
      "Epoch = 120, loss = 23.56918004155159\n",
      "j = 321, Train Acc = 0.9553571428571429, Test Acc = 0.9539473684210527\n",
      "j = 250, Train Acc = 0.9099025974025974, Test Acc = 0.9177631578947368\n",
      "j = 200, Train Acc = 0.885551948051948, Test Acc = 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "### training parameters\n",
    "\n",
    "insize=X_train.size(2)\n",
    "hsize=64\n",
    "outsize=2    #binary classification\n",
    "model = matchRNN(insize,hsize,outsize)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size=16\n",
    "num_batches = int(X_train.size(0) / batch_size)\n",
    "\n",
    "# train iterations\n",
    "for epoch in range(121):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch_wickets.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 121, loss = 23.294075459241867\n",
      "Epoch = 122, loss = 24.023326262831688\n",
      "Epoch = 123, loss = 24.001443095505238\n",
      "Epoch = 124, loss = 23.641899898648262\n",
      "Epoch = 125, loss = 23.27492828667164\n",
      "Epoch = 126, loss = 23.223719894886017\n",
      "Epoch = 127, loss = 23.302832826972008\n",
      "Epoch = 128, loss = 23.307702481746674\n",
      "Epoch = 129, loss = 23.283289924263954\n",
      "Epoch = 130, loss = 23.056348279118538\n",
      "j = 321, Train Acc = 0.9626623376623377, Test Acc = 0.9638157894736842\n",
      "j = 250, Train Acc = 0.9090909090909091, Test Acc = 0.9243421052631579\n",
      "j = 200, Train Acc = 0.8871753246753247, Test Acc = 0.881578947368421\n",
      "Epoch = 131, loss = 23.02853734791279\n",
      "Epoch = 132, loss = 23.043737798929214\n",
      "Epoch = 133, loss = 23.00191716849804\n",
      "Epoch = 134, loss = 23.05367349088192\n",
      "Epoch = 135, loss = 22.999273493885994\n",
      "Epoch = 136, loss = 22.89139437675476\n",
      "Epoch = 137, loss = 22.964842692017555\n",
      "Epoch = 138, loss = 22.902574479579926\n",
      "Epoch = 139, loss = 22.920948386192322\n",
      "Epoch = 140, loss = 22.790304243564606\n",
      "j = 321, Train Acc = 0.9594155844155844, Test Acc = 0.9539473684210527\n",
      "j = 250, Train Acc = 0.9107142857142857, Test Acc = 0.9177631578947368\n",
      "j = 200, Train Acc = 0.8887987012987013, Test Acc = 0.8881578947368421\n",
      "Epoch = 141, loss = 22.95434172451496\n",
      "Epoch = 142, loss = 22.82522402703762\n",
      "Epoch = 143, loss = 22.834129616618156\n",
      "Epoch = 144, loss = 22.833589419722557\n",
      "Epoch = 145, loss = 22.99971902370453\n",
      "Epoch = 146, loss = 22.768903508782387\n",
      "Epoch = 147, loss = 22.687206760048866\n",
      "Epoch = 148, loss = 22.769721388816833\n",
      "Epoch = 149, loss = 22.582935333251953\n",
      "Epoch = 150, loss = 23.183214411139488\n",
      "j = 321, Train Acc = 0.9594155844155844, Test Acc = 0.9539473684210527\n",
      "j = 250, Train Acc = 0.9155844155844156, Test Acc = 0.9144736842105263\n",
      "j = 200, Train Acc = 0.8871753246753247, Test Acc = 0.881578947368421\n",
      "Epoch = 151, loss = 23.763994112610817\n",
      "Epoch = 152, loss = 23.617226660251617\n",
      "Epoch = 153, loss = 23.095616549253464\n",
      "Epoch = 154, loss = 22.9805825650692\n",
      "Epoch = 155, loss = 22.977940186858177\n",
      "Epoch = 156, loss = 23.151885360479355\n",
      "Epoch = 157, loss = 22.68424467742443\n",
      "Epoch = 158, loss = 22.668310925364494\n",
      "Epoch = 159, loss = 22.68523618578911\n",
      "Epoch = 160, loss = 22.523082733154297\n",
      "j = 321, Train Acc = 0.9577922077922078, Test Acc = 0.9506578947368421\n",
      "j = 250, Train Acc = 0.9163961038961039, Test Acc = 0.9046052631578947\n",
      "j = 200, Train Acc = 0.887987012987013, Test Acc = 0.8881578947368421\n",
      "Epoch = 161, loss = 22.50204487144947\n",
      "Epoch = 162, loss = 22.438775569200516\n",
      "Epoch = 163, loss = 22.627454698085785\n",
      "Epoch = 164, loss = 22.512160405516624\n",
      "Epoch = 165, loss = 22.420625180006027\n",
      "Epoch = 166, loss = 22.354457423090935\n",
      "Epoch = 167, loss = 22.46878443658352\n",
      "Epoch = 168, loss = 22.469541773200035\n",
      "Epoch = 169, loss = 22.59736116230488\n",
      "Epoch = 170, loss = 23.24629621207714\n",
      "j = 321, Train Acc = 0.9545454545454546, Test Acc = 0.9407894736842105\n",
      "j = 250, Train Acc = 0.9188311688311688, Test Acc = 0.9046052631578947\n",
      "j = 200, Train Acc = 0.8887987012987013, Test Acc = 0.8881578947368421\n",
      "Epoch = 171, loss = 22.30992019176483\n",
      "Epoch = 172, loss = 22.283169090747833\n",
      "Epoch = 173, loss = 22.529758974909782\n",
      "Epoch = 174, loss = 22.415422469377518\n",
      "Epoch = 175, loss = 22.244389042258263\n",
      "Epoch = 176, loss = 22.31923870742321\n",
      "Epoch = 177, loss = 22.38545224070549\n",
      "Epoch = 178, loss = 22.410529866814613\n",
      "Epoch = 179, loss = 22.594126984477043\n",
      "Epoch = 180, loss = 22.08633168041706\n",
      "j = 321, Train Acc = 0.9707792207792207, Test Acc = 0.9572368421052632\n",
      "j = 250, Train Acc = 0.9123376623376623, Test Acc = 0.9111842105263158\n",
      "j = 200, Train Acc = 0.8920454545454546, Test Acc = 0.881578947368421\n",
      "Epoch = 181, loss = 22.106152325868607\n",
      "Epoch = 182, loss = 22.21591980755329\n",
      "Epoch = 183, loss = 22.216951683163643\n",
      "Epoch = 184, loss = 22.25270016491413\n",
      "Epoch = 185, loss = 24.78630840778351\n",
      "Epoch = 186, loss = 22.275638967752457\n",
      "Epoch = 187, loss = 22.544831335544586\n",
      "Epoch = 188, loss = 22.145415917038918\n",
      "Epoch = 189, loss = 22.23894515633583\n",
      "Epoch = 190, loss = 21.98975646495819\n",
      "j = 321, Train Acc = 0.9715909090909091, Test Acc = 0.9605263157894737\n",
      "j = 250, Train Acc = 0.9188311688311688, Test Acc = 0.9144736842105263\n",
      "j = 200, Train Acc = 0.8920454545454546, Test Acc = 0.881578947368421\n",
      "Epoch = 191, loss = 22.155547738075256\n",
      "Epoch = 192, loss = 22.18144017457962\n",
      "Epoch = 193, loss = 22.127630457282066\n",
      "Epoch = 194, loss = 21.887894958257675\n",
      "Epoch = 195, loss = 22.095013812184334\n",
      "Epoch = 196, loss = 21.873732447624207\n",
      "Epoch = 197, loss = 21.980743169784546\n",
      "Epoch = 198, loss = 24.19373247027397\n",
      "Epoch = 199, loss = 22.559907719492912\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(121,201):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch_wickets.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, loss = 21.613310262560844\n",
      "j = 321, Train Acc = 0.9642857142857143, Test Acc = 0.9605263157894737\n",
      "j = 250, Train Acc = 0.9196428571428571, Test Acc = 0.9078947368421053\n",
      "j = 200, Train Acc = 0.8912337662337663, Test Acc = 0.8848684210526315\n",
      "j = 150, Train Acc = 0.8612012987012987, Test Acc = 0.875\n",
      "j = 100, Train Acc = 0.8011363636363636, Test Acc = 0.8618421052631579\n",
      "j = 60, Train Acc = 0.788961038961039, Test Acc = 0.819078947368421\n",
      "j = 30, Train Acc = 0.6818181818181818, Test Acc = 0.7072368421052632\n",
      "j = 6, Train Acc = 0.510551948051948, Test Acc = 0.5164473684210527\n",
      "Epoch = 1, loss = 21.765536531805992\n",
      "j = 321, Train Acc = 0.9732142857142857, Test Acc = 0.9671052631578947\n",
      "j = 250, Train Acc = 0.9180194805194806, Test Acc = 0.930921052631579\n",
      "j = 200, Train Acc = 0.898538961038961, Test Acc = 0.881578947368421\n",
      "j = 150, Train Acc = 0.8587662337662337, Test Acc = 0.8848684210526315\n",
      "j = 100, Train Acc = 0.8011363636363636, Test Acc = 0.8355263157894737\n",
      "j = 60, Train Acc = 0.7873376623376623, Test Acc = 0.8256578947368421\n",
      "j = 30, Train Acc = 0.6907467532467533, Test Acc = 0.7368421052631579\n",
      "j = 6, Train Acc = 0.5178571428571429, Test Acc = 0.5263157894736842\n",
      "Epoch = 2, loss = 21.66558825969696\n",
      "j = 321, Train Acc = 0.9732142857142857, Test Acc = 0.9572368421052632\n",
      "j = 250, Train Acc = 0.9204545454545454, Test Acc = 0.9177631578947368\n",
      "j = 200, Train Acc = 0.8944805194805194, Test Acc = 0.875\n",
      "j = 150, Train Acc = 0.8587662337662337, Test Acc = 0.868421052631579\n",
      "j = 100, Train Acc = 0.8035714285714286, Test Acc = 0.8453947368421053\n",
      "j = 60, Train Acc = 0.7857142857142857, Test Acc = 0.8223684210526315\n",
      "j = 30, Train Acc = 0.6777597402597403, Test Acc = 0.7039473684210527\n",
      "j = 6, Train Acc = 0.5170454545454546, Test Acc = 0.5230263157894737\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "\n",
    "    for j in [321,250,200,150,100,60,30,6]:\n",
    "            evalfunc(model,j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results on multi output lstm\n",
    "\n",
    "# Epoch = 80, loss = 42.69335952401161\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9473684210526315\n",
    "# j = 250, Train Acc = 0.7021103896103896, Test Acc = 0.7401315789473685\n",
    "# j = 200, Train Acc = 0.6323051948051948, Test Acc = 0.6381578947368421\n",
    "# Epoch = 81, loss = 42.64999434351921\n",
    "# Epoch = 82, loss = 42.63620883226395\n",
    "# Epoch = 83, loss = 42.57252901792526\n",
    "# Epoch = 84, loss = 42.54386180639267\n",
    "# Epoch = 85, loss = 42.543820798397064\n",
    "# Epoch = 86, loss = 42.522236466407776\n",
    "# Epoch = 87, loss = 43.12302175164223\n",
    "# Epoch = 88, loss = 43.80488169193268\n",
    "# Epoch = 89, loss = 43.530466586351395\n",
    "# Epoch = 90, loss = 43.1253487765789\n",
    "# j = 321, Train Acc = 0.9042207792207793, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.6996753246753247, Test Acc = 0.7467105263157895\n",
    "# j = 200, Train Acc = 0.635551948051948, Test Acc = 0.6644736842105263\n",
    "# Epoch = 91, loss = 42.85170575976372\n",
    "# Epoch = 92, loss = 42.611751973629\n",
    "# Epoch = 93, loss = 42.60165584087372\n",
    "# Epoch = 94, loss = 42.472452610731125\n",
    "# Epoch = 95, loss = 42.44126981496811\n",
    "# Epoch = 96, loss = 42.43537795543671\n",
    "# Epoch = 97, loss = 42.41048192977905\n",
    "# Epoch = 98, loss = 42.40796732902527\n",
    "# Epoch = 99, loss = 42.37926670908928\n",
    "# Epoch = 100, loss = 42.36977231502533\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9572368421052632\n",
    "# j = 250, Train Acc = 0.7021103896103896, Test Acc = 0.743421052631579\n",
    "# j = 200, Train Acc = 0.635551948051948, Test Acc = 0.6578947368421053\n",
    "# Epoch = 101, loss = 42.35787692666054\n",
    "# Epoch = 102, loss = 42.33479583263397\n",
    "# Epoch = 103, loss = 42.33716815710068\n",
    "# Epoch = 104, loss = 42.291197776794434\n",
    "# Epoch = 105, loss = 42.42200693488121\n",
    "# Epoch = 106, loss = 42.384660959243774\n",
    "# Epoch = 107, loss = 42.503537118434906\n",
    "# Epoch = 108, loss = 42.54160389304161\n",
    "# Epoch = 109, loss = 42.657554507255554\n",
    "# Epoch = 110, loss = 46.20248129963875\n",
    "# j = 321, Train Acc = 0.5048701298701299, Test Acc = 0.5526315789473685\n",
    "# j = 250, Train Acc = 0.5876623376623377, Test Acc = 0.6052631578947368\n",
    "# j = 200, Train Acc = 0.5909090909090909, Test Acc = 0.5953947368421053\n",
    "# Epoch = 111, loss = 49.134992361068726\n",
    "# Epoch = 112, loss = 43.54741933941841\n",
    "# Epoch = 113, loss = 42.86708441376686\n",
    "# Epoch = 114, loss = 42.59751954674721\n",
    "# Epoch = 115, loss = 42.5811333656311\n",
    "# Epoch = 116, loss = 42.51113286614418\n",
    "# Epoch = 117, loss = 42.68782064318657\n",
    "# Epoch = 118, loss = 42.618944466114044\n",
    "# Epoch = 119, loss = 42.34784010052681\n",
    "# Epoch = 120, loss = 42.301506608724594\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.713474025974026, Test Acc = 0.75\n",
    "# j = 200, Train Acc = 0.650974025974026, Test Acc = 0.6710526315789473\n",
    "# Epoch = 121, loss = 42.29225406050682\n",
    "# Epoch = 122, loss = 42.695088654756546\n",
    "# Epoch = 123, loss = 42.92030057311058\n",
    "# Epoch = 124, loss = 42.58220049738884\n",
    "# Epoch = 125, loss = 42.21531546115875\n",
    "# Epoch = 126, loss = 42.28876554965973\n",
    "# Epoch = 127, loss = 42.07214707136154\n",
    "# Epoch = 128, loss = 42.15156552195549\n",
    "# Epoch = 129, loss = 42.09659793972969\n",
    "# Epoch = 130, loss = 43.25178563594818\n",
    "# j = 321, Train Acc = 0.8522727272727273, Test Acc = 0.8322368421052632\n",
    "# j = 250, Train Acc = 0.6566558441558441, Test Acc = 0.6414473684210527\n",
    "# j = 200, Train Acc = 0.6112012987012987, Test Acc = 0.5822368421052632\n",
    "# Epoch = 131, loss = 46.67593550682068\n",
    "# Epoch = 132, loss = 43.80374363064766\n",
    "# Epoch = 133, loss = 42.56290856003761\n",
    "# Epoch = 134, loss = 42.68502974510193\n",
    "# Epoch = 135, loss = 42.1684812605381\n",
    "# Epoch = 136, loss = 42.16793215274811\n",
    "# Epoch = 137, loss = 42.16465583443642\n",
    "# Epoch = 138, loss = 42.15301898121834\n",
    "# Epoch = 139, loss = 41.93092507123947\n",
    "# Epoch = 140, loss = 42.10533806681633\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7167207792207793, Test Acc = 0.75\n",
    "# j = 200, Train Acc = 0.6550324675324676, Test Acc = 0.6743421052631579\n",
    "# Epoch = 141, loss = 41.746142119169235\n",
    "# Epoch = 142, loss = 42.21582242846489\n",
    "# Epoch = 143, loss = 41.93819710612297\n",
    "# Epoch = 144, loss = 41.87680941820145\n",
    "# Epoch = 145, loss = 42.27157709002495\n",
    "# Epoch = 146, loss = 41.94843155145645\n",
    "# Epoch = 147, loss = 41.99655598402023\n",
    "# Epoch = 148, loss = 42.1709089577198\n",
    "# Epoch = 149, loss = 42.353795766830444\n",
    "# Epoch = 150, loss = 42.91742631793022\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7126623376623377, Test Acc = 0.7105263157894737\n",
    "# j = 200, Train Acc = 0.6501623376623377, Test Acc = 0.6217105263157895\n",
    "# Epoch = 151, loss = 42.41686064004898\n",
    "# Epoch = 152, loss = 42.26151829957962\n",
    "# Epoch = 153, loss = 42.90386986732483\n",
    "# Epoch = 154, loss = 42.91596955060959\n",
    "# Epoch = 155, loss = 42.06308516860008\n",
    "# Epoch = 156, loss = 42.01913532614708\n",
    "# Epoch = 157, loss = 41.886956721544266\n",
    "# Epoch = 158, loss = 42.28912091255188\n",
    "# Epoch = 159, loss = 41.89504021406174\n",
    "# Epoch = 160, loss = 41.59468686580658\n",
    "# j = 321, Train Acc = 0.9058441558441559, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7224025974025974, Test Acc = 0.7368421052631579\n",
    "# j = 200, Train Acc = 0.6574675324675324, Test Acc = 0.6447368421052632\n",
    "# Epoch = 161, loss = 42.02638882398605\n",
    "# Epoch = 162, loss = 42.76215770840645\n",
    "# Epoch = 163, loss = 41.943300515413284\n",
    "# Epoch = 164, loss = 41.744627594947815\n",
    "# Epoch = 165, loss = 41.64369750022888\n",
    "# Epoch = 166, loss = 41.82700064778328\n",
    "# Epoch = 167, loss = 41.838323920965195\n",
    "# Epoch = 168, loss = 42.99936231970787\n",
    "# Epoch = 169, loss = 42.55475810170174\n",
    "# Epoch = 170, loss = 42.43961876630783\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.724025974025974, Test Acc = 0.7236842105263158\n",
    "# j = 200, Train Acc = 0.6525974025974026, Test Acc = 0.6513157894736842\n",
    "# Epoch = 171, loss = 42.14726781845093\n",
    "# Epoch = 172, loss = 42.02729895710945\n",
    "# Epoch = 173, loss = 42.05796667933464\n",
    "# Epoch = 174, loss = 42.047207325696945\n",
    "# Epoch = 175, loss = 41.85863038897514\n",
    "# Epoch = 176, loss = 41.78750139474869\n",
    "# Epoch = 177, loss = 41.89787817001343\n",
    "# Epoch = 178, loss = 41.723336696624756\n",
    "# Epoch = 179, loss = 41.46990704536438\n",
    "# Epoch = 180, loss = 41.31174489855766\n",
    "# j = 321, Train Acc = 0.9147727272727273, Test Acc = 0.9539473684210527\n",
    "# j = 250, Train Acc = 0.7386363636363636, Test Acc = 0.743421052631579\n",
    "# j = 200, Train Acc = 0.6728896103896104, Test Acc = 0.6546052631578947\n",
    "# Epoch = 181, loss = 41.147144973278046\n",
    "# Epoch = 182, loss = 41.04915153980255\n",
    "# Epoch = 183, loss = 41.02901268005371\n",
    "# Epoch = 184, loss = 41.16104966402054\n",
    "# Epoch = 185, loss = 41.052144914865494\n",
    "# Epoch = 186, loss = 40.75493836402893\n",
    "# Epoch = 187, loss = 40.8061888217926\n",
    "# Epoch = 188, loss = 41.45923164486885\n",
    "# Epoch = 189, loss = 40.893089562654495\n",
    "# Epoch = 190, loss = 40.89757114648819\n",
    "# j = 321, Train Acc = 0.9188311688311688, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.734577922077922, Test Acc = 0.7138157894736842\n",
    "# j = 200, Train Acc = 0.6728896103896104, Test Acc = 0.6447368421052632\n",
    "# Epoch = 191, loss = 40.91097801923752\n",
    "# Epoch = 192, loss = 40.533461928367615\n",
    "# Epoch = 193, loss = 40.0526128411293\n",
    "# Epoch = 194, loss = 40.267620861530304\n",
    "# Epoch = 195, loss = 40.43658027052879\n",
    "# Epoch = 196, loss = 39.8879688680172\n",
    "# Epoch = 197, loss = 40.29349622130394\n",
    "# Epoch = 198, loss = 39.77166989445686\n",
    "# Epoch = 199, loss = 40.42837768793106\n",
    "# Epoch = 200, loss = 39.85610529780388\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7564935064935064, Test Acc = 0.7105263157894737\n",
    "# j = 200, Train Acc = 0.698051948051948, Test Acc = 0.6381578947368421"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
