{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and process data\n",
    "path = r'odi_csv/'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "shuffle(all_files)\n",
    "innings2 = []\n",
    "targ_score = []\n",
    "target=[]\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename,usecols=[0])\n",
    "    skips = df.loc[ 'info' , : ].shape[0]\n",
    "\n",
    "    df = pd.read_csv(filename,nrows=skips,skiprows=1,header=None)\n",
    "    df = df.drop(columns=0).set_index(df.columns[1])\n",
    "    winteam=None\n",
    "    if 'winner' in df.index:\n",
    "        winteam = df.loc['winner',:].values[0]\n",
    "    \n",
    "    df = pd.read_csv(filename,skiprows=skips+1,header=None)\n",
    "    df2 = df[df.columns[[1,2,7,8,9]]].set_index(df.columns[1]).drop(index=1)\n",
    "    df2[9] = df2[9].notna() * 1\n",
    "    df2[9] = df2[9].cumsum()\n",
    "    df1_ = df[df.columns[[1,2,7,8]]].set_index(df.columns[1])\n",
    "    df1 = df1_[df1_.index==1]\n",
    "\n",
    "    if df2.shape[0]>0:\n",
    "        innings2.append(df2)\n",
    "        targ_score.append(df1[df1.columns[[1,2]]].sum().sum())\n",
    "        i2team = df[df.columns[[1,3]]].set_index(df.columns[1]).drop(index=1).values[0,0]\n",
    "        if (i2team==winteam):\n",
    "            target.append(1)\n",
    "        else:\n",
    "            target.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1244, 321, 4])\n",
      "torch.Size([312, 321, 4])\n"
     ]
    }
   ],
   "source": [
    "# torch tensor processing\n",
    "features=[]\n",
    "# create targets first\n",
    "for i in range(len(innings2)):\n",
    "    features.append(torch.tensor(innings2[i].values))\n",
    "\n",
    "\n",
    "# convert to fixed length sequence\n",
    "features = pad_sequence(features,batch_first=True, padding_value=-1)\n",
    "targets = torch.tensor(target)\n",
    "\n",
    "split = int(len(features) * 0.8)\n",
    "X_train = features[:split]\n",
    "X_test  = features[split:]\n",
    "y_train = targets[:split]\n",
    "y_test  = targets[split:]\n",
    "print(X_train.size())\n",
    "print(X_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1244, 321, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_score_train = np.transpose(targ_score[:split] * np.ones((1,X_train.shape[1],X_train.shape[0])))\n",
    "targ_score_test = np.transpose(targ_score[split:] * np.ones((1,X_test.shape[1],X_test.shape[0])))\n",
    "X_train = torch.cat((X_train.float(), torch.Tensor(targ_score_train)), 2)\n",
    "X_test = torch.cat((X_test.float(), torch.Tensor(targ_score_test)), 2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "class matchRNN(nn.Module):\n",
    "    def __init__(self,insize,hsize,outsize):\n",
    "        super(matchRNN,self).__init__()\n",
    "        \n",
    "        self.insize=insize\n",
    "        self.hsize=hsize\n",
    "        self.outsize = outsize\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=insize, hidden_size=hsize)\n",
    "        self.fc_out = nn.Linear(in_features=hsize, out_features=outsize)\n",
    "#         self.dropout = nn.Dropout(p=0.2, inplace=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,feat):\n",
    "#         feat = torch.tensor(feat[np.newaxis,:],dtype=torch.float32)\n",
    "        batch_size = feat.size(0)\n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hsize))\n",
    "        cell_state = torch.zeros((batch_size, self.hsize))\n",
    "        outputs = torch.empty((batch_size, feat.size(1), self.outsize))\n",
    "\n",
    "        for t in range(feat.size(1)):\n",
    "\n",
    "            # for the first time step (if input is different)\n",
    "            if t == 0:\n",
    "                hidden_state, cell_state = self.lstm_cell(feat[:,t,:].view(batch_size,-1).float(), (hidden_state, cell_state))\n",
    "                \n",
    "            # for the 2nd+ time step\n",
    "            else:\n",
    "                hidden_state, cell_state = self.lstm_cell(feat[:,t,:].view(batch_size,-1).float(), (hidden_state, cell_state))\n",
    "            \n",
    "#             dropouts = self.dropout(hidden_state)\n",
    "            out = self.fc_out(hidden_state)\n",
    "#             out = self.softmax(out)\n",
    "            outputs[:,t,:] = out\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1244, 321])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain_tiled = y_train.repeat(X_train.size(1),1).transpose(0,1)\n",
    "ytrain_tiled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalfunc(model,j):\n",
    "    model.eval()\n",
    "    # train\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    corr=0\n",
    "    num_batches = X.size(0) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        op = model(X[start_idx:end_idx])\n",
    "        req_op = op[:,j-1]\n",
    "        maxval,maxidx = torch.max(req_op,1)\n",
    "        corr+= np.sum((maxidx==y[start_idx:end_idx]).numpy())\n",
    "    total=num_batches*batch_size\n",
    "    train_acc = corr / total\n",
    "    \n",
    "    # test\n",
    "    X = X_test\n",
    "    y = y_test\n",
    "    corr=0\n",
    "    num_batches = X.size(0) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        op = model(X[start_idx:end_idx])\n",
    "        req_op = op[:,j-1]\n",
    "        maxval,maxidx = torch.max(req_op,1)\n",
    "        corr+= np.sum((maxidx==y[start_idx:end_idx]).numpy())\n",
    "    total=num_batches*batch_size\n",
    "    test_acc = corr / total\n",
    "    print('j = {}, Train Acc = {}, Test Acc = {}'.format(j,train_acc,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, loss = 49.317942798137665\n",
      "j = 321, Train Acc = 0.6793831168831169, Test Acc = 0.7039473684210527\n",
      "j = 250, Train Acc = 0.6712662337662337, Test Acc = 0.6842105263157895\n",
      "j = 200, Train Acc = 0.6956168831168831, Test Acc = 0.7138157894736842\n",
      "Epoch = 1, loss = 43.19525820016861\n",
      "Epoch = 2, loss = 40.69996976852417\n",
      "Epoch = 3, loss = 39.09889575839043\n",
      "Epoch = 4, loss = 37.816164433956146\n",
      "Epoch = 5, loss = 36.7632494866848\n",
      "Epoch = 6, loss = 36.079714983701706\n",
      "Epoch = 7, loss = 35.1954450905323\n",
      "Epoch = 8, loss = 34.48428435623646\n",
      "Epoch = 9, loss = 33.74219496548176\n",
      "Epoch = 10, loss = 33.01973010599613\n",
      "j = 321, Train Acc = 0.827922077922078, Test Acc = 0.819078947368421\n",
      "j = 250, Train Acc = 0.8441558441558441, Test Acc = 0.8519736842105263\n",
      "j = 200, Train Acc = 0.8214285714285714, Test Acc = 0.8256578947368421\n",
      "Epoch = 11, loss = 32.34846006333828\n",
      "Epoch = 12, loss = 31.66387005150318\n",
      "Epoch = 13, loss = 31.193733483552933\n",
      "Epoch = 14, loss = 30.374895632267\n",
      "Epoch = 15, loss = 30.256094947457314\n",
      "Epoch = 16, loss = 29.533255383372307\n",
      "Epoch = 17, loss = 29.135960698127747\n",
      "Epoch = 18, loss = 28.83803839981556\n",
      "Epoch = 19, loss = 28.47526741027832\n",
      "Epoch = 20, loss = 28.146742716431618\n",
      "j = 321, Train Acc = 0.9180194805194806, Test Acc = 0.8980263157894737\n",
      "j = 250, Train Acc = 0.8733766233766234, Test Acc = 0.8618421052631579\n",
      "j = 200, Train Acc = 0.8271103896103896, Test Acc = 0.8322368421052632\n",
      "Epoch = 21, loss = 27.88352544605732\n",
      "Epoch = 22, loss = 27.652830690145493\n",
      "Epoch = 23, loss = 27.46561147272587\n",
      "Epoch = 24, loss = 27.224708318710327\n",
      "Epoch = 25, loss = 27.16664458811283\n",
      "Epoch = 26, loss = 26.85682475566864\n",
      "Epoch = 27, loss = 26.869107738137245\n",
      "Epoch = 28, loss = 26.557418659329414\n",
      "Epoch = 29, loss = 26.59009651839733\n",
      "Epoch = 30, loss = 26.42539557814598\n",
      "j = 321, Train Acc = 0.9155844155844156, Test Acc = 0.9046052631578947\n",
      "j = 250, Train Acc = 0.8758116883116883, Test Acc = 0.8519736842105263\n",
      "j = 200, Train Acc = 0.836038961038961, Test Acc = 0.8322368421052632\n",
      "Epoch = 31, loss = 26.348707899451256\n",
      "Epoch = 32, loss = 26.227708160877228\n",
      "Epoch = 33, loss = 26.17678715288639\n",
      "Epoch = 34, loss = 26.131599843502045\n",
      "Epoch = 35, loss = 26.029018089175224\n",
      "Epoch = 36, loss = 25.976268216967583\n",
      "Epoch = 37, loss = 25.90147729218006\n",
      "Epoch = 38, loss = 25.895826533436775\n",
      "Epoch = 39, loss = 25.882492512464523\n",
      "Epoch = 40, loss = 25.79867598414421\n",
      "j = 321, Train Acc = 0.9123376623376623, Test Acc = 0.8980263157894737\n",
      "j = 250, Train Acc = 0.8790584415584416, Test Acc = 0.8585526315789473\n",
      "j = 200, Train Acc = 0.8465909090909091, Test Acc = 0.8421052631578947\n",
      "Epoch = 41, loss = 25.757984057068825\n",
      "Epoch = 42, loss = 25.691524848341942\n",
      "Epoch = 43, loss = 25.658214330673218\n",
      "Epoch = 44, loss = 25.63792395591736\n",
      "Epoch = 45, loss = 25.54777915775776\n",
      "Epoch = 46, loss = 25.549387454986572\n",
      "Epoch = 47, loss = 25.463864773511887\n",
      "Epoch = 48, loss = 25.438156977295876\n",
      "Epoch = 49, loss = 25.42678889632225\n",
      "Epoch = 50, loss = 25.303429260849953\n",
      "j = 321, Train Acc = 0.9147727272727273, Test Acc = 0.9013157894736842\n",
      "j = 250, Train Acc = 0.877435064935065, Test Acc = 0.8585526315789473\n",
      "j = 200, Train Acc = 0.8474025974025974, Test Acc = 0.8421052631578947\n",
      "Epoch = 51, loss = 25.34630709886551\n",
      "Epoch = 52, loss = 25.25416086614132\n",
      "Epoch = 53, loss = 25.272703900933266\n",
      "Epoch = 54, loss = 25.2643251568079\n",
      "Epoch = 55, loss = 25.240441665053368\n",
      "Epoch = 56, loss = 25.179824352264404\n",
      "Epoch = 57, loss = 25.12145435810089\n",
      "Epoch = 58, loss = 25.09811407327652\n",
      "Epoch = 59, loss = 25.05427595973015\n",
      "Epoch = 60, loss = 25.04215033352375\n",
      "j = 321, Train Acc = 0.9180194805194806, Test Acc = 0.9013157894736842\n",
      "j = 250, Train Acc = 0.8733766233766234, Test Acc = 0.8618421052631579\n",
      "j = 200, Train Acc = 0.8457792207792207, Test Acc = 0.8355263157894737\n",
      "Epoch = 61, loss = 24.99387925863266\n",
      "Epoch = 62, loss = 24.97840577363968\n",
      "Epoch = 63, loss = 24.95629869401455\n",
      "Epoch = 64, loss = 25.11317989230156\n",
      "Epoch = 65, loss = 24.867866441607475\n",
      "Epoch = 66, loss = 24.95388574898243\n",
      "Epoch = 67, loss = 24.797523096203804\n",
      "Epoch = 68, loss = 24.764090344309807\n",
      "Epoch = 69, loss = 24.734977021813393\n",
      "Epoch = 70, loss = 24.768503338098526\n",
      "j = 321, Train Acc = 0.9301948051948052, Test Acc = 0.9078947368421053\n",
      "j = 250, Train Acc = 0.8733766233766234, Test Acc = 0.8618421052631579\n",
      "j = 200, Train Acc = 0.8457792207792207, Test Acc = 0.8388157894736842\n",
      "Epoch = 71, loss = 24.787631943821907\n",
      "Epoch = 72, loss = 24.67999118566513\n",
      "Epoch = 73, loss = 24.713565349578857\n",
      "Epoch = 74, loss = 24.597035869956017\n",
      "Epoch = 75, loss = 24.600130572915077\n",
      "Epoch = 76, loss = 24.57357583940029\n",
      "Epoch = 77, loss = 24.60005423426628\n",
      "Epoch = 78, loss = 24.619892477989197\n",
      "Epoch = 79, loss = 24.623507529497147\n",
      "Epoch = 80, loss = 24.62640331685543\n",
      "j = 321, Train Acc = 0.9237012987012987, Test Acc = 0.9046052631578947\n",
      "j = 250, Train Acc = 0.8741883116883117, Test Acc = 0.8552631578947368\n",
      "j = 200, Train Acc = 0.8425324675324676, Test Acc = 0.8355263157894737\n",
      "Epoch = 81, loss = 24.441024780273438\n",
      "Epoch = 82, loss = 24.5530798882246\n",
      "Epoch = 83, loss = 24.54172395169735\n",
      "Epoch = 84, loss = 24.396460935473442\n",
      "Epoch = 85, loss = 24.517368927598\n",
      "Epoch = 86, loss = 24.510087579488754\n",
      "Epoch = 87, loss = 24.566112115979195\n",
      "Epoch = 88, loss = 24.4638399630785\n",
      "Epoch = 89, loss = 24.446833565831184\n",
      "Epoch = 90, loss = 24.25823265314102\n",
      "j = 321, Train Acc = 0.9375, Test Acc = 0.9210526315789473\n",
      "j = 250, Train Acc = 0.8798701298701299, Test Acc = 0.868421052631579\n",
      "j = 200, Train Acc = 0.8449675324675324, Test Acc = 0.8322368421052632\n",
      "Epoch = 91, loss = 24.316211462020874\n",
      "Epoch = 92, loss = 24.294559985399246\n",
      "Epoch = 93, loss = 24.44614215195179\n",
      "Epoch = 94, loss = 24.363238006830215\n",
      "Epoch = 95, loss = 24.24287724494934\n",
      "Epoch = 96, loss = 24.201555997133255\n",
      "Epoch = 97, loss = 24.33375909924507\n",
      "Epoch = 98, loss = 24.177666321396828\n",
      "Epoch = 99, loss = 24.215534701943398\n",
      "Epoch = 100, loss = 24.34679064154625\n",
      "j = 321, Train Acc = 0.9358766233766234, Test Acc = 0.9111842105263158\n",
      "j = 250, Train Acc = 0.875, Test Acc = 0.8519736842105263\n",
      "j = 200, Train Acc = 0.8425324675324676, Test Acc = 0.8322368421052632\n",
      "Epoch = 101, loss = 24.486229613423347\n",
      "Epoch = 102, loss = 24.267463624477386\n",
      "Epoch = 103, loss = 24.3653222322464\n",
      "Epoch = 104, loss = 24.246180534362793\n",
      "Epoch = 105, loss = 24.264029413461685\n",
      "Epoch = 106, loss = 24.95573218166828\n",
      "Epoch = 107, loss = 25.001708835363388\n",
      "Epoch = 108, loss = 24.32315780222416\n",
      "Epoch = 109, loss = 24.44937475025654\n",
      "Epoch = 110, loss = 24.469198167324066\n",
      "j = 321, Train Acc = 0.9253246753246753, Test Acc = 0.9013157894736842\n",
      "j = 250, Train Acc = 0.8668831168831169, Test Acc = 0.8552631578947368\n",
      "j = 200, Train Acc = 0.8425324675324676, Test Acc = 0.8322368421052632\n",
      "Epoch = 111, loss = 24.420151114463806\n",
      "Epoch = 112, loss = 24.240778729319572\n",
      "Epoch = 113, loss = 24.269990295171738\n",
      "Epoch = 114, loss = 24.23074345290661\n",
      "Epoch = 115, loss = 24.062615394592285\n",
      "Epoch = 116, loss = 24.04195438325405\n",
      "Epoch = 117, loss = 24.33146744966507\n",
      "Epoch = 118, loss = 24.82858471572399\n",
      "Epoch = 119, loss = 24.621311455965042\n",
      "Epoch = 120, loss = 24.409963741898537\n",
      "j = 321, Train Acc = 0.9375, Test Acc = 0.9210526315789473\n",
      "j = 250, Train Acc = 0.877435064935065, Test Acc = 0.8552631578947368\n",
      "j = 200, Train Acc = 0.8538961038961039, Test Acc = 0.8256578947368421\n",
      "Epoch = 121, loss = 24.237506166100502\n",
      "Epoch = 122, loss = 23.998330250382423\n",
      "Epoch = 123, loss = 24.934640109539032\n",
      "Epoch = 124, loss = 23.96827606856823\n",
      "Epoch = 125, loss = 23.88410533964634\n",
      "Epoch = 126, loss = 24.108252733945847\n",
      "Epoch = 127, loss = 23.887128621339798\n",
      "Epoch = 128, loss = 23.84702156484127\n",
      "Epoch = 129, loss = 23.882317379117012\n",
      "Epoch = 130, loss = 23.785444855690002\n",
      "j = 321, Train Acc = 0.9472402597402597, Test Acc = 0.930921052631579\n",
      "j = 250, Train Acc = 0.8766233766233766, Test Acc = 0.8552631578947368\n",
      "j = 200, Train Acc = 0.8474025974025974, Test Acc = 0.8289473684210527\n"
     ]
    }
   ],
   "source": [
    "### training parameters\n",
    "\n",
    "insize=X_train.size(2)\n",
    "hsize=64\n",
    "outsize=2    #binary classification\n",
    "model = matchRNN(insize,hsize,outsize)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size=16\n",
    "num_batches = int(X_train.size(0) / batch_size)\n",
    "\n",
    "# train iterations\n",
    "for epoch in range(131):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch_wickets_cumsum.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 131, loss = 23.811761111021042\n",
      "Epoch = 132, loss = 23.81314317882061\n",
      "Epoch = 133, loss = 23.78511181473732\n",
      "Epoch = 134, loss = 24.20979245007038\n",
      "Epoch = 135, loss = 23.72346141934395\n",
      "Epoch = 136, loss = 23.7836002856493\n",
      "Epoch = 137, loss = 23.751746982336044\n",
      "Epoch = 138, loss = 24.43377213180065\n",
      "Epoch = 139, loss = 24.577778056263924\n",
      "Epoch = 140, loss = 23.939472645521164\n",
      "j = 321, Train Acc = 0.9415584415584416, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.877435064935065, Test Acc = 0.8486842105263158\n",
      "j = 200, Train Acc = 0.8498376623376623, Test Acc = 0.8388157894736842\n",
      "Epoch = 141, loss = 24.08524616062641\n",
      "Epoch = 142, loss = 23.649434089660645\n",
      "Epoch = 143, loss = 25.398348450660706\n",
      "Epoch = 144, loss = 23.72163026034832\n",
      "Epoch = 145, loss = 23.658362552523613\n",
      "Epoch = 146, loss = 23.534914508461952\n",
      "Epoch = 147, loss = 24.06657938659191\n",
      "Epoch = 148, loss = 23.518991753458977\n",
      "Epoch = 149, loss = 23.668445020914078\n",
      "Epoch = 150, loss = 23.504086315631866\n",
      "j = 321, Train Acc = 0.9391233766233766, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.8758116883116883, Test Acc = 0.8552631578947368\n",
      "j = 200, Train Acc = 0.8482142857142857, Test Acc = 0.8289473684210527\n",
      "Epoch = 151, loss = 23.542869299650192\n",
      "Epoch = 152, loss = 23.571482479572296\n",
      "Epoch = 153, loss = 23.52550306916237\n",
      "Epoch = 154, loss = 24.35721166431904\n",
      "Epoch = 155, loss = 23.55064883828163\n",
      "Epoch = 156, loss = 23.49479942023754\n",
      "Epoch = 157, loss = 23.789404287934303\n",
      "Epoch = 158, loss = 24.201425671577454\n",
      "Epoch = 159, loss = 23.77709013223648\n",
      "Epoch = 160, loss = 23.657112509012222\n",
      "j = 321, Train Acc = 0.939935064935065, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.877435064935065, Test Acc = 0.8519736842105263\n",
      "j = 200, Train Acc = 0.8449675324675324, Test Acc = 0.8355263157894737\n",
      "Epoch = 161, loss = 23.280936539173126\n",
      "Epoch = 162, loss = 23.40615926682949\n",
      "Epoch = 163, loss = 23.555999383330345\n",
      "Epoch = 164, loss = 23.646751686930656\n",
      "Epoch = 165, loss = 23.684469044208527\n",
      "Epoch = 166, loss = 23.375546544790268\n",
      "Epoch = 167, loss = 24.1834699511528\n",
      "Epoch = 168, loss = 23.957195594906807\n",
      "Epoch = 169, loss = 23.290566638112068\n",
      "Epoch = 170, loss = 23.530951872467995\n",
      "j = 321, Train Acc = 0.9448051948051948, Test Acc = 0.9243421052631579\n",
      "j = 250, Train Acc = 0.8823051948051948, Test Acc = 0.8519736842105263\n",
      "j = 200, Train Acc = 0.8522727272727273, Test Acc = 0.8355263157894737\n",
      "Epoch = 171, loss = 23.415094450116158\n",
      "Epoch = 172, loss = 23.39367711544037\n",
      "Epoch = 173, loss = 23.60189165174961\n",
      "Epoch = 174, loss = 23.38986237347126\n",
      "Epoch = 175, loss = 23.341901198029518\n",
      "Epoch = 176, loss = 23.801464289426804\n",
      "Epoch = 177, loss = 23.446716278791428\n",
      "Epoch = 178, loss = 23.250552847981453\n",
      "Epoch = 179, loss = 23.586260110139847\n",
      "Epoch = 180, loss = 24.65872411429882\n",
      "j = 321, Train Acc = 0.9407467532467533, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.8766233766233766, Test Acc = 0.8552631578947368\n",
      "j = 200, Train Acc = 0.8441558441558441, Test Acc = 0.8256578947368421\n",
      "Epoch = 181, loss = 23.43753692507744\n",
      "Epoch = 182, loss = 23.643500044941902\n",
      "Epoch = 183, loss = 23.495939940214157\n",
      "Epoch = 184, loss = 23.416669934988022\n",
      "Epoch = 185, loss = 23.842220664024353\n",
      "Epoch = 186, loss = 23.181661292910576\n",
      "Epoch = 187, loss = 23.415019065141678\n",
      "Epoch = 188, loss = 23.34801958501339\n",
      "Epoch = 189, loss = 23.26409089565277\n",
      "Epoch = 190, loss = 23.42827707529068\n",
      "j = 321, Train Acc = 0.9383116883116883, Test Acc = 0.8980263157894737\n",
      "j = 250, Train Acc = 0.8733766233766234, Test Acc = 0.8388157894736842\n",
      "j = 200, Train Acc = 0.8482142857142857, Test Acc = 0.8322368421052632\n",
      "Epoch = 191, loss = 23.85608559846878\n",
      "Epoch = 192, loss = 23.64893151819706\n",
      "Epoch = 193, loss = 23.408084213733673\n",
      "Epoch = 194, loss = 23.23856881260872\n",
      "Epoch = 195, loss = 23.182076424360275\n",
      "Epoch = 196, loss = 23.040740936994553\n",
      "Epoch = 197, loss = 23.294104173779488\n",
      "Epoch = 198, loss = 23.248056650161743\n",
      "Epoch = 199, loss = 28.82916697859764\n",
      "Epoch = 200, loss = 24.61689691245556\n",
      "j = 321, Train Acc = 0.9488636363636364, Test Acc = 0.9243421052631579\n",
      "j = 250, Train Acc = 0.8912337662337663, Test Acc = 0.8618421052631579\n",
      "j = 200, Train Acc = 0.8612012987012987, Test Acc = 0.8453947368421053\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(131,201):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch_wickets_cumsum.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 201, loss = 23.880692273378372\n",
      "Epoch = 202, loss = 23.552158921957016\n",
      "Epoch = 203, loss = 23.398915395140648\n",
      "Epoch = 204, loss = 23.365433245897293\n",
      "Epoch = 205, loss = 23.390290021896362\n",
      "Epoch = 206, loss = 23.368431702256203\n",
      "Epoch = 207, loss = 23.391541838645935\n",
      "Epoch = 208, loss = 23.293615996837616\n",
      "Epoch = 209, loss = 23.314920872449875\n",
      "Epoch = 210, loss = 23.227320969104767\n",
      "j = 321, Train Acc = 0.9407467532467533, Test Acc = 0.9046052631578947\n",
      "j = 250, Train Acc = 0.8766233766233766, Test Acc = 0.8651315789473685\n",
      "j = 200, Train Acc = 0.8547077922077922, Test Acc = 0.8289473684210527\n",
      "Epoch = 211, loss = 23.268429785966873\n",
      "Epoch = 212, loss = 23.362068727612495\n",
      "Epoch = 213, loss = 23.32781173288822\n",
      "Epoch = 214, loss = 24.2028985619545\n",
      "Epoch = 215, loss = 23.774260014295578\n",
      "Epoch = 216, loss = 23.462967321276665\n",
      "Epoch = 217, loss = 23.19710521399975\n",
      "Epoch = 218, loss = 23.34289312362671\n",
      "Epoch = 219, loss = 23.212772235274315\n",
      "Epoch = 220, loss = 23.28747072815895\n",
      "j = 321, Train Acc = 0.9415584415584416, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.8782467532467533, Test Acc = 0.8585526315789473\n",
      "j = 200, Train Acc = 0.851461038961039, Test Acc = 0.8322368421052632\n",
      "Epoch = 221, loss = 23.86378738284111\n",
      "Epoch = 222, loss = 23.853007405996323\n",
      "Epoch = 223, loss = 23.40766668319702\n",
      "Epoch = 224, loss = 23.628236338496208\n",
      "Epoch = 225, loss = 23.113183185458183\n",
      "Epoch = 226, loss = 23.85627892613411\n",
      "Epoch = 227, loss = 23.079656705260277\n",
      "Epoch = 228, loss = 23.16060207784176\n",
      "Epoch = 229, loss = 23.13231059908867\n",
      "Epoch = 230, loss = 23.22330540418625\n",
      "j = 321, Train Acc = 0.9415584415584416, Test Acc = 0.9144736842105263\n",
      "j = 250, Train Acc = 0.8798701298701299, Test Acc = 0.8618421052631579\n",
      "j = 200, Train Acc = 0.8441558441558441, Test Acc = 0.8322368421052632\n",
      "Epoch = 231, loss = 23.198392003774643\n",
      "Epoch = 232, loss = 23.93509116768837\n",
      "Epoch = 233, loss = 23.32906274497509\n",
      "Epoch = 234, loss = 23.354095458984375\n",
      "Epoch = 235, loss = 23.34264561533928\n",
      "Epoch = 236, loss = 23.042306661605835\n",
      "Epoch = 237, loss = 23.196645513176918\n",
      "Epoch = 238, loss = 23.128208577632904\n",
      "Epoch = 239, loss = 23.115755885839462\n",
      "Epoch = 240, loss = 23.081313207745552\n",
      "j = 321, Train Acc = 0.9423701298701299, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.8798701298701299, Test Acc = 0.8618421052631579\n",
      "j = 200, Train Acc = 0.8482142857142857, Test Acc = 0.8322368421052632\n",
      "Epoch = 241, loss = 23.051152303814888\n",
      "Epoch = 242, loss = 23.12520119547844\n",
      "Epoch = 243, loss = 23.0764377117157\n",
      "Epoch = 244, loss = 23.239967197179794\n",
      "Epoch = 245, loss = 23.120258271694183\n",
      "Epoch = 246, loss = 23.072548627853394\n",
      "Epoch = 247, loss = 23.28526322543621\n",
      "Epoch = 248, loss = 23.063100561499596\n",
      "Epoch = 249, loss = 23.05402560532093\n",
      "Epoch = 250, loss = 23.128577694296837\n",
      "j = 321, Train Acc = 0.9456168831168831, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.8798701298701299, Test Acc = 0.8585526315789473\n",
      "j = 200, Train Acc = 0.8555194805194806, Test Acc = 0.8256578947368421\n",
      "Epoch = 251, loss = 23.04662999510765\n",
      "Epoch = 252, loss = 23.362844794988632\n",
      "Epoch = 253, loss = 23.052078306674957\n",
      "Epoch = 254, loss = 23.06379969418049\n",
      "Epoch = 255, loss = 23.056051790714264\n",
      "Epoch = 256, loss = 23.062543392181396\n",
      "Epoch = 257, loss = 22.971899330615997\n",
      "Epoch = 258, loss = 23.059754997491837\n",
      "Epoch = 259, loss = 22.975092127919197\n",
      "Epoch = 260, loss = 23.02042020857334\n",
      "j = 321, Train Acc = 0.9456168831168831, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.8839285714285714, Test Acc = 0.868421052631579\n",
      "j = 200, Train Acc = 0.8547077922077922, Test Acc = 0.8289473684210527\n",
      "Epoch = 261, loss = 23.00275355577469\n",
      "Epoch = 262, loss = 23.128384992480278\n",
      "Epoch = 263, loss = 22.993194580078125\n",
      "Epoch = 264, loss = 22.978006303310394\n",
      "Epoch = 265, loss = 23.198142051696777\n",
      "Epoch = 266, loss = 22.919395372271538\n",
      "Epoch = 267, loss = 23.182152897119522\n",
      "Epoch = 268, loss = 22.981257751584053\n",
      "Epoch = 269, loss = 22.900220066308975\n",
      "Epoch = 270, loss = 22.9768757969141\n",
      "j = 321, Train Acc = 0.948051948051948, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.8847402597402597, Test Acc = 0.8585526315789473\n",
      "j = 200, Train Acc = 0.8522727272727273, Test Acc = 0.8223684210526315\n",
      "Epoch = 271, loss = 22.862975984811783\n",
      "Epoch = 272, loss = 22.93551868200302\n",
      "Epoch = 273, loss = 22.861300066113472\n",
      "Epoch = 274, loss = 22.905122324824333\n",
      "Epoch = 275, loss = 22.85855360329151\n",
      "Epoch = 276, loss = 22.889504492282867\n",
      "Epoch = 277, loss = 22.848394572734833\n",
      "Epoch = 278, loss = 22.87897276878357\n",
      "Epoch = 279, loss = 22.824323251843452\n",
      "Epoch = 280, loss = 22.873174980282784\n",
      "j = 321, Train Acc = 0.948051948051948, Test Acc = 0.9111842105263158\n",
      "j = 250, Train Acc = 0.8863636363636364, Test Acc = 0.8651315789473685\n",
      "j = 200, Train Acc = 0.8547077922077922, Test Acc = 0.8355263157894737\n",
      "Epoch = 281, loss = 22.931629434227943\n",
      "Epoch = 282, loss = 22.91329801082611\n",
      "Epoch = 283, loss = 22.858018174767494\n",
      "Epoch = 284, loss = 22.857264950871468\n",
      "Epoch = 285, loss = 22.795338466763496\n",
      "Epoch = 286, loss = 22.749655678868294\n",
      "Epoch = 287, loss = 22.824309200048447\n",
      "Epoch = 288, loss = 22.726679995656013\n",
      "Epoch = 289, loss = 22.791486278176308\n",
      "Epoch = 290, loss = 22.79791484773159\n",
      "j = 321, Train Acc = 0.9488636363636364, Test Acc = 0.9111842105263158\n",
      "j = 250, Train Acc = 0.8887987012987013, Test Acc = 0.8585526315789473\n",
      "j = 200, Train Acc = 0.849025974025974, Test Acc = 0.8223684210526315\n",
      "Epoch = 291, loss = 23.04683232307434\n",
      "Epoch = 292, loss = 22.99887515604496\n",
      "Epoch = 293, loss = 22.73768849670887\n",
      "Epoch = 294, loss = 22.82270050048828\n",
      "Epoch = 295, loss = 22.759806215763092\n",
      "Epoch = 296, loss = 22.739995226264\n",
      "Epoch = 297, loss = 22.848063752055168\n",
      "Epoch = 298, loss = 22.72779206931591\n",
      "Epoch = 299, loss = 22.68359887599945\n",
      "Epoch = 300, loss = 22.794695988297462\n",
      "j = 321, Train Acc = 0.9521103896103896, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.8887987012987013, Test Acc = 0.8519736842105263\n",
      "j = 200, Train Acc = 0.8538961038961039, Test Acc = 0.8355263157894737\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(201,301):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch_wickets_cumsum.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results on multi output lstm\n",
    "\n",
    "# Epoch = 80, loss = 42.69335952401161\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9473684210526315\n",
    "# j = 250, Train Acc = 0.7021103896103896, Test Acc = 0.7401315789473685\n",
    "# j = 200, Train Acc = 0.6323051948051948, Test Acc = 0.6381578947368421\n",
    "# Epoch = 81, loss = 42.64999434351921\n",
    "# Epoch = 82, loss = 42.63620883226395\n",
    "# Epoch = 83, loss = 42.57252901792526\n",
    "# Epoch = 84, loss = 42.54386180639267\n",
    "# Epoch = 85, loss = 42.543820798397064\n",
    "# Epoch = 86, loss = 42.522236466407776\n",
    "# Epoch = 87, loss = 43.12302175164223\n",
    "# Epoch = 88, loss = 43.80488169193268\n",
    "# Epoch = 89, loss = 43.530466586351395\n",
    "# Epoch = 90, loss = 43.1253487765789\n",
    "# j = 321, Train Acc = 0.9042207792207793, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.6996753246753247, Test Acc = 0.7467105263157895\n",
    "# j = 200, Train Acc = 0.635551948051948, Test Acc = 0.6644736842105263\n",
    "# Epoch = 91, loss = 42.85170575976372\n",
    "# Epoch = 92, loss = 42.611751973629\n",
    "# Epoch = 93, loss = 42.60165584087372\n",
    "# Epoch = 94, loss = 42.472452610731125\n",
    "# Epoch = 95, loss = 42.44126981496811\n",
    "# Epoch = 96, loss = 42.43537795543671\n",
    "# Epoch = 97, loss = 42.41048192977905\n",
    "# Epoch = 98, loss = 42.40796732902527\n",
    "# Epoch = 99, loss = 42.37926670908928\n",
    "# Epoch = 100, loss = 42.36977231502533\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9572368421052632\n",
    "# j = 250, Train Acc = 0.7021103896103896, Test Acc = 0.743421052631579\n",
    "# j = 200, Train Acc = 0.635551948051948, Test Acc = 0.6578947368421053\n",
    "# Epoch = 101, loss = 42.35787692666054\n",
    "# Epoch = 102, loss = 42.33479583263397\n",
    "# Epoch = 103, loss = 42.33716815710068\n",
    "# Epoch = 104, loss = 42.291197776794434\n",
    "# Epoch = 105, loss = 42.42200693488121\n",
    "# Epoch = 106, loss = 42.384660959243774\n",
    "# Epoch = 107, loss = 42.503537118434906\n",
    "# Epoch = 108, loss = 42.54160389304161\n",
    "# Epoch = 109, loss = 42.657554507255554\n",
    "# Epoch = 110, loss = 46.20248129963875\n",
    "# j = 321, Train Acc = 0.5048701298701299, Test Acc = 0.5526315789473685\n",
    "# j = 250, Train Acc = 0.5876623376623377, Test Acc = 0.6052631578947368\n",
    "# j = 200, Train Acc = 0.5909090909090909, Test Acc = 0.5953947368421053\n",
    "# Epoch = 111, loss = 49.134992361068726\n",
    "# Epoch = 112, loss = 43.54741933941841\n",
    "# Epoch = 113, loss = 42.86708441376686\n",
    "# Epoch = 114, loss = 42.59751954674721\n",
    "# Epoch = 115, loss = 42.5811333656311\n",
    "# Epoch = 116, loss = 42.51113286614418\n",
    "# Epoch = 117, loss = 42.68782064318657\n",
    "# Epoch = 118, loss = 42.618944466114044\n",
    "# Epoch = 119, loss = 42.34784010052681\n",
    "# Epoch = 120, loss = 42.301506608724594\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.713474025974026, Test Acc = 0.75\n",
    "# j = 200, Train Acc = 0.650974025974026, Test Acc = 0.6710526315789473\n",
    "# Epoch = 121, loss = 42.29225406050682\n",
    "# Epoch = 122, loss = 42.695088654756546\n",
    "# Epoch = 123, loss = 42.92030057311058\n",
    "# Epoch = 124, loss = 42.58220049738884\n",
    "# Epoch = 125, loss = 42.21531546115875\n",
    "# Epoch = 126, loss = 42.28876554965973\n",
    "# Epoch = 127, loss = 42.07214707136154\n",
    "# Epoch = 128, loss = 42.15156552195549\n",
    "# Epoch = 129, loss = 42.09659793972969\n",
    "# Epoch = 130, loss = 43.25178563594818\n",
    "# j = 321, Train Acc = 0.8522727272727273, Test Acc = 0.8322368421052632\n",
    "# j = 250, Train Acc = 0.6566558441558441, Test Acc = 0.6414473684210527\n",
    "# j = 200, Train Acc = 0.6112012987012987, Test Acc = 0.5822368421052632\n",
    "# Epoch = 131, loss = 46.67593550682068\n",
    "# Epoch = 132, loss = 43.80374363064766\n",
    "# Epoch = 133, loss = 42.56290856003761\n",
    "# Epoch = 134, loss = 42.68502974510193\n",
    "# Epoch = 135, loss = 42.1684812605381\n",
    "# Epoch = 136, loss = 42.16793215274811\n",
    "# Epoch = 137, loss = 42.16465583443642\n",
    "# Epoch = 138, loss = 42.15301898121834\n",
    "# Epoch = 139, loss = 41.93092507123947\n",
    "# Epoch = 140, loss = 42.10533806681633\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7167207792207793, Test Acc = 0.75\n",
    "# j = 200, Train Acc = 0.6550324675324676, Test Acc = 0.6743421052631579\n",
    "# Epoch = 141, loss = 41.746142119169235\n",
    "# Epoch = 142, loss = 42.21582242846489\n",
    "# Epoch = 143, loss = 41.93819710612297\n",
    "# Epoch = 144, loss = 41.87680941820145\n",
    "# Epoch = 145, loss = 42.27157709002495\n",
    "# Epoch = 146, loss = 41.94843155145645\n",
    "# Epoch = 147, loss = 41.99655598402023\n",
    "# Epoch = 148, loss = 42.1709089577198\n",
    "# Epoch = 149, loss = 42.353795766830444\n",
    "# Epoch = 150, loss = 42.91742631793022\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7126623376623377, Test Acc = 0.7105263157894737\n",
    "# j = 200, Train Acc = 0.6501623376623377, Test Acc = 0.6217105263157895\n",
    "# Epoch = 151, loss = 42.41686064004898\n",
    "# Epoch = 152, loss = 42.26151829957962\n",
    "# Epoch = 153, loss = 42.90386986732483\n",
    "# Epoch = 154, loss = 42.91596955060959\n",
    "# Epoch = 155, loss = 42.06308516860008\n",
    "# Epoch = 156, loss = 42.01913532614708\n",
    "# Epoch = 157, loss = 41.886956721544266\n",
    "# Epoch = 158, loss = 42.28912091255188\n",
    "# Epoch = 159, loss = 41.89504021406174\n",
    "# Epoch = 160, loss = 41.59468686580658\n",
    "# j = 321, Train Acc = 0.9058441558441559, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7224025974025974, Test Acc = 0.7368421052631579\n",
    "# j = 200, Train Acc = 0.6574675324675324, Test Acc = 0.6447368421052632\n",
    "# Epoch = 161, loss = 42.02638882398605\n",
    "# Epoch = 162, loss = 42.76215770840645\n",
    "# Epoch = 163, loss = 41.943300515413284\n",
    "# Epoch = 164, loss = 41.744627594947815\n",
    "# Epoch = 165, loss = 41.64369750022888\n",
    "# Epoch = 166, loss = 41.82700064778328\n",
    "# Epoch = 167, loss = 41.838323920965195\n",
    "# Epoch = 168, loss = 42.99936231970787\n",
    "# Epoch = 169, loss = 42.55475810170174\n",
    "# Epoch = 170, loss = 42.43961876630783\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.724025974025974, Test Acc = 0.7236842105263158\n",
    "# j = 200, Train Acc = 0.6525974025974026, Test Acc = 0.6513157894736842\n",
    "# Epoch = 171, loss = 42.14726781845093\n",
    "# Epoch = 172, loss = 42.02729895710945\n",
    "# Epoch = 173, loss = 42.05796667933464\n",
    "# Epoch = 174, loss = 42.047207325696945\n",
    "# Epoch = 175, loss = 41.85863038897514\n",
    "# Epoch = 176, loss = 41.78750139474869\n",
    "# Epoch = 177, loss = 41.89787817001343\n",
    "# Epoch = 178, loss = 41.723336696624756\n",
    "# Epoch = 179, loss = 41.46990704536438\n",
    "# Epoch = 180, loss = 41.31174489855766\n",
    "# j = 321, Train Acc = 0.9147727272727273, Test Acc = 0.9539473684210527\n",
    "# j = 250, Train Acc = 0.7386363636363636, Test Acc = 0.743421052631579\n",
    "# j = 200, Train Acc = 0.6728896103896104, Test Acc = 0.6546052631578947\n",
    "# Epoch = 181, loss = 41.147144973278046\n",
    "# Epoch = 182, loss = 41.04915153980255\n",
    "# Epoch = 183, loss = 41.02901268005371\n",
    "# Epoch = 184, loss = 41.16104966402054\n",
    "# Epoch = 185, loss = 41.052144914865494\n",
    "# Epoch = 186, loss = 40.75493836402893\n",
    "# Epoch = 187, loss = 40.8061888217926\n",
    "# Epoch = 188, loss = 41.45923164486885\n",
    "# Epoch = 189, loss = 40.893089562654495\n",
    "# Epoch = 190, loss = 40.89757114648819\n",
    "# j = 321, Train Acc = 0.9188311688311688, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.734577922077922, Test Acc = 0.7138157894736842\n",
    "# j = 200, Train Acc = 0.6728896103896104, Test Acc = 0.6447368421052632\n",
    "# Epoch = 191, loss = 40.91097801923752\n",
    "# Epoch = 192, loss = 40.533461928367615\n",
    "# Epoch = 193, loss = 40.0526128411293\n",
    "# Epoch = 194, loss = 40.267620861530304\n",
    "# Epoch = 195, loss = 40.43658027052879\n",
    "# Epoch = 196, loss = 39.8879688680172\n",
    "# Epoch = 197, loss = 40.29349622130394\n",
    "# Epoch = 198, loss = 39.77166989445686\n",
    "# Epoch = 199, loss = 40.42837768793106\n",
    "# Epoch = 200, loss = 39.85610529780388\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7564935064935064, Test Acc = 0.7105263157894737\n",
    "# j = 200, Train Acc = 0.698051948051948, Test Acc = 0.6381578947368421"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
