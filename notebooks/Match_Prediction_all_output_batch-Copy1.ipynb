{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load and process data\n",
    "path = r'odi_csv/'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "shuffle(all_files)\n",
    "innings2 = []\n",
    "target=[]\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename,usecols=[0])\n",
    "    skips = df.loc[ 'info' , : ].shape[0]\n",
    "\n",
    "    df = pd.read_csv(filename,nrows=skips,skiprows=1,header=None)\n",
    "    df = df.drop(columns=0).set_index(df.columns[1])\n",
    "    winteam=None\n",
    "    if 'winner' in df.index:\n",
    "        winteam = df.loc['winner',:].values[0]\n",
    "    \n",
    "    df = pd.read_csv(filename,skiprows=skips+1,header=None)\n",
    "    \n",
    "    df2 = df[df.columns[[1,2,7,8]]].set_index(df.columns[1]).drop(index=1)\n",
    "    if df2.shape[0]>0:\n",
    "        innings2.append(df2)\n",
    "        i2team = df[df.columns[[1,3]]].set_index(df.columns[1]).drop(index=1).values[0,0]\n",
    "        if (i2team==winteam):\n",
    "            target.append(1)\n",
    "        else:\n",
    "            target.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1244, 321, 3])\n",
      "torch.Size([312, 321, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch tensor processing\n",
    "features=[]\n",
    "# create targets first\n",
    "for i in range(len(innings2)):\n",
    "    features.append(torch.tensor(innings2[i].values))\n",
    "\n",
    "\n",
    "# convert to fixed length sequence\n",
    "features = pad_sequence(features,batch_first=True, padding_value=-1)\n",
    "targets = torch.tensor(target)\n",
    "\n",
    "split = int(len(features) * 0.8)\n",
    "X_train = features[:split]\n",
    "X_test  = features[split:]\n",
    "y_train = targets[:split]\n",
    "y_test  = targets[split:]\n",
    "print(X_train.size())\n",
    "print(X_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "class matchRNN(nn.Module):\n",
    "    def __init__(self,insize,hsize,outsize):\n",
    "        super(matchRNN,self).__init__()\n",
    "        \n",
    "        self.insize=insize\n",
    "        self.hsize=hsize\n",
    "        self.outsize = outsize\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=insize, hidden_size=hsize)\n",
    "        self.fc_out = nn.Linear(in_features=hsize, out_features=outsize)\n",
    "#         self.dropout = nn.Dropout(p=0.2, inplace=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,feat):\n",
    "#         feat = torch.tensor(feat[np.newaxis,:],dtype=torch.float32)\n",
    "        batch_size = feat.size(0)\n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hsize))\n",
    "        cell_state = torch.zeros((batch_size, self.hsize))\n",
    "        outputs = torch.empty((batch_size, feat.size(1), self.outsize))\n",
    "\n",
    "        for t in range(feat.size(1)):\n",
    "\n",
    "            # for the first time step (if input is different)\n",
    "            if t == 0:\n",
    "                hidden_state, cell_state = self.lstm_cell(feat[:,t,:].view(batch_size,-1).float(), (hidden_state, cell_state))\n",
    "                \n",
    "            # for the 2nd+ time step\n",
    "            else:\n",
    "                hidden_state, cell_state = self.lstm_cell(feat[:,t,:].view(batch_size,-1).float(), (hidden_state, cell_state))\n",
    "            \n",
    "#             dropouts = self.dropout(hidden_state)\n",
    "            out = self.fc_out(hidden_state)\n",
    "#             out = self.softmax(out)\n",
    "            outputs[:,t,:] = out\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1244, 321])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain_tiled = y_train.repeat(X_train.size(1),1).transpose(0,1)\n",
    "ytrain_tiled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalfunc(model,j):\n",
    "    model.eval()\n",
    "    # train\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    corr=0\n",
    "    num_batches = X.size(0) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        op = model(X[start_idx:end_idx])\n",
    "        req_op = op[:,j-1]\n",
    "        maxval,maxidx = torch.max(req_op,1)\n",
    "        corr+= np.sum((maxidx==y[start_idx:end_idx]).numpy())\n",
    "    total=num_batches*batch_size\n",
    "    train_acc = corr / total\n",
    "    \n",
    "    # test\n",
    "    X = X_test\n",
    "    y = y_test\n",
    "    corr=0\n",
    "    num_batches = X.size(0) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        op = model(X[start_idx:end_idx])\n",
    "        req_op = op[:,j-1]\n",
    "        maxval,maxidx = torch.max(req_op,1)\n",
    "        corr+= np.sum((maxidx==y[start_idx:end_idx]).numpy())\n",
    "    total=num_batches*batch_size\n",
    "    test_acc = corr / total\n",
    "    print('j = {}, Train Acc = {}, Test Acc = {}'.format(j,train_acc,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, loss = 53.343428790569305\n",
      "j = 321, Train Acc = 0.5081168831168831, Test Acc = 0.4934210526315789\n",
      "j = 250, Train Acc = 0.5673701298701299, Test Acc = 0.5296052631578947\n",
      "j = 200, Train Acc = 0.5397727272727273, Test Acc = 0.5164473684210527\n",
      "Epoch = 1, loss = 53.15908634662628\n",
      "Epoch = 2, loss = 52.940744280815125\n",
      "Epoch = 3, loss = 53.07577192783356\n",
      "Epoch = 4, loss = 51.74091190099716\n",
      "Epoch = 5, loss = 49.93058007955551\n",
      "Epoch = 6, loss = 48.537143528461456\n",
      "Epoch = 7, loss = 47.5353062748909\n",
      "Epoch = 8, loss = 48.90459868311882\n",
      "Epoch = 9, loss = 48.8341081738472\n",
      "Epoch = 10, loss = 47.947246730327606\n",
      "j = 321, Train Acc = 0.827922077922078, Test Acc = 0.8256578947368421\n",
      "j = 250, Train Acc = 0.6436688311688312, Test Acc = 0.6118421052631579\n",
      "j = 200, Train Acc = 0.5641233766233766, Test Acc = 0.5986842105263158\n",
      "Epoch = 11, loss = 48.34395134449005\n",
      "Epoch = 12, loss = 48.712593257427216\n",
      "Epoch = 13, loss = 47.57472413778305\n",
      "Epoch = 14, loss = 53.58564209938049\n",
      "Epoch = 15, loss = 53.14413517713547\n",
      "Epoch = 16, loss = 53.09477561712265\n",
      "Epoch = 17, loss = 53.046340227127075\n",
      "Epoch = 18, loss = 52.99846035242081\n",
      "Epoch = 19, loss = 52.84647411108017\n",
      "Epoch = 20, loss = 50.52174985408783\n",
      "j = 321, Train Acc = 0.5081168831168831, Test Acc = 0.4934210526315789\n",
      "j = 250, Train Acc = 0.5349025974025974, Test Acc = 0.5164473684210527\n",
      "j = 200, Train Acc = 0.5073051948051948, Test Acc = 0.4934210526315789\n",
      "Epoch = 21, loss = 53.23903024196625\n",
      "Epoch = 22, loss = 53.13801950216293\n",
      "Epoch = 23, loss = 53.12156921625137\n",
      "Epoch = 24, loss = 53.10721892118454\n",
      "Epoch = 25, loss = 53.08736830949783\n",
      "Epoch = 26, loss = 53.07385855913162\n",
      "Epoch = 27, loss = 53.05585664510727\n",
      "Epoch = 28, loss = 53.03273528814316\n",
      "Epoch = 29, loss = 52.963062167167664\n",
      "Epoch = 30, loss = 52.94953817129135\n",
      "j = 321, Train Acc = 0.5081168831168831, Test Acc = 0.4934210526315789\n",
      "j = 250, Train Acc = 0.5657467532467533, Test Acc = 0.5296052631578947\n",
      "j = 200, Train Acc = 0.5422077922077922, Test Acc = 0.5197368421052632\n",
      "Epoch = 31, loss = 53.13383638858795\n",
      "Epoch = 32, loss = 53.09364002943039\n",
      "Epoch = 33, loss = 53.06413662433624\n",
      "Epoch = 34, loss = 53.02802860736847\n",
      "Epoch = 35, loss = 52.995430171489716\n",
      "Epoch = 36, loss = 52.96322864294052\n",
      "Epoch = 37, loss = 52.88109505176544\n",
      "Epoch = 38, loss = 52.687863886356354\n",
      "Epoch = 39, loss = 50.50362157821655\n",
      "Epoch = 40, loss = 48.486430525779724\n",
      "j = 321, Train Acc = 0.8733766233766234, Test Acc = 0.8157894736842105\n",
      "j = 250, Train Acc = 0.6623376623376623, Test Acc = 0.5888157894736842\n",
      "j = 200, Train Acc = 0.5795454545454546, Test Acc = 0.569078947368421\n",
      "Epoch = 41, loss = 47.48479348421097\n",
      "Epoch = 42, loss = 46.393082678318024\n",
      "Epoch = 43, loss = 46.30511802434921\n",
      "Epoch = 44, loss = 46.38653689622879\n",
      "Epoch = 45, loss = 45.721176356077194\n",
      "Epoch = 46, loss = 45.7097589969635\n",
      "Epoch = 47, loss = 46.50807252526283\n",
      "Epoch = 48, loss = 47.754431784152985\n",
      "Epoch = 49, loss = 50.116500437259674\n",
      "Epoch = 50, loss = 48.94139975309372\n",
      "j = 321, Train Acc = 0.8116883116883117, Test Acc = 0.8223684210526315\n",
      "j = 250, Train Acc = 0.6258116883116883, Test Acc = 0.6414473684210527\n",
      "j = 200, Train Acc = 0.5576298701298701, Test Acc = 0.5789473684210527\n",
      "Epoch = 51, loss = 48.721910655498505\n",
      "Epoch = 52, loss = 48.6135990023613\n",
      "Epoch = 53, loss = 48.29542136192322\n",
      "Epoch = 54, loss = 47.92733657360077\n",
      "Epoch = 55, loss = 47.71558040380478\n",
      "Epoch = 56, loss = 47.66215431690216\n",
      "Epoch = 57, loss = 47.895410895347595\n",
      "Epoch = 58, loss = 47.31455239653587\n",
      "Epoch = 59, loss = 47.47664397954941\n",
      "Epoch = 60, loss = 47.372606068849564\n",
      "j = 321, Train Acc = 0.8417207792207793, Test Acc = 0.8453947368421053\n",
      "j = 250, Train Acc = 0.6444805194805194, Test Acc = 0.6381578947368421\n",
      "j = 200, Train Acc = 0.6258116883116883, Test Acc = 0.625\n",
      "Epoch = 61, loss = 51.03951048851013\n",
      "Epoch = 62, loss = 50.68430268764496\n",
      "Epoch = 63, loss = 47.972720086574554\n",
      "Epoch = 64, loss = 46.956469655036926\n",
      "Epoch = 65, loss = 46.022264897823334\n",
      "Epoch = 66, loss = 48.912818133831024\n",
      "Epoch = 67, loss = 46.21657109260559\n",
      "Epoch = 68, loss = 46.14930132031441\n",
      "Epoch = 69, loss = 45.794744193553925\n",
      "Epoch = 70, loss = 45.15475243330002\n",
      "j = 321, Train Acc = 0.9180194805194806, Test Acc = 0.881578947368421\n",
      "j = 250, Train Acc = 0.6842532467532467, Test Acc = 0.631578947368421\n",
      "j = 200, Train Acc = 0.5803571428571429, Test Acc = 0.618421052631579\n",
      "Epoch = 71, loss = 44.91354784369469\n",
      "Epoch = 72, loss = 44.83454990386963\n",
      "Epoch = 73, loss = 44.19099074602127\n",
      "Epoch = 74, loss = 43.96252003312111\n",
      "Epoch = 75, loss = 44.42180210351944\n",
      "Epoch = 76, loss = 44.21847638487816\n",
      "Epoch = 77, loss = 43.801281213760376\n",
      "Epoch = 78, loss = 43.774485886096954\n",
      "Epoch = 79, loss = 43.83814549446106\n",
      "Epoch = 80, loss = 43.429866909980774\n",
      "j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9013157894736842\n",
      "j = 250, Train Acc = 0.7061688311688312, Test Acc = 0.7039473684210527\n",
      "j = 200, Train Acc = 0.6542207792207793, Test Acc = 0.6414473684210527\n",
      "Epoch = 81, loss = 43.70638680458069\n",
      "Epoch = 82, loss = 44.1051179766655\n",
      "Epoch = 83, loss = 44.24182844161987\n",
      "Epoch = 84, loss = 43.783143788576126\n",
      "Epoch = 85, loss = 43.70749980211258\n",
      "Epoch = 86, loss = 43.66128897666931\n",
      "Epoch = 87, loss = 43.95647644996643\n",
      "Epoch = 88, loss = 43.77631142735481\n",
      "Epoch = 89, loss = 43.431713342666626\n",
      "Epoch = 90, loss = 43.15262672305107\n",
      "j = 321, Train Acc = 0.9090909090909091, Test Acc = 0.8980263157894737\n",
      "j = 250, Train Acc = 0.7037337662337663, Test Acc = 0.6776315789473685\n",
      "j = 200, Train Acc = 0.6112012987012987, Test Acc = 0.6447368421052632\n",
      "Epoch = 91, loss = 43.060414999723434\n",
      "Epoch = 92, loss = 42.982090681791306\n",
      "Epoch = 93, loss = 42.9513658285141\n",
      "Epoch = 94, loss = 42.88432660698891\n",
      "Epoch = 95, loss = 42.86597290635109\n",
      "Epoch = 96, loss = 42.81218904256821\n",
      "Epoch = 97, loss = 42.750164568424225\n",
      "Epoch = 98, loss = 42.8616498708725\n",
      "Epoch = 99, loss = 42.79568761587143\n",
      "Epoch = 100, loss = 42.75914180278778\n",
      "j = 321, Train Acc = 0.9163961038961039, Test Acc = 0.9144736842105263\n",
      "j = 250, Train Acc = 0.7061688311688312, Test Acc = 0.7302631578947368\n",
      "j = 200, Train Acc = 0.6452922077922078, Test Acc = 0.6447368421052632\n",
      "Epoch = 101, loss = 42.71124267578125\n",
      "Epoch = 102, loss = 42.674002796411514\n",
      "Epoch = 103, loss = 42.65320047736168\n",
      "Epoch = 104, loss = 42.54101479053497\n",
      "Epoch = 105, loss = 43.23944181203842\n",
      "Epoch = 106, loss = 42.50831499695778\n",
      "Epoch = 107, loss = 42.466223388910294\n",
      "Epoch = 108, loss = 42.43252521753311\n",
      "Epoch = 109, loss = 42.395019829273224\n",
      "Epoch = 110, loss = 42.39851677417755\n",
      "j = 321, Train Acc = 0.9163961038961039, Test Acc = 0.9144736842105263\n",
      "j = 250, Train Acc = 0.7086038961038961, Test Acc = 0.7171052631578947\n",
      "j = 200, Train Acc = 0.6477272727272727, Test Acc = 0.6513157894736842\n",
      "Epoch = 111, loss = 42.34183257818222\n",
      "Epoch = 112, loss = 42.3683799803257\n",
      "Epoch = 113, loss = 44.07662150263786\n",
      "Epoch = 114, loss = 45.065193235874176\n",
      "Epoch = 115, loss = 42.956997871398926\n",
      "Epoch = 116, loss = 42.85164678096771\n",
      "Epoch = 117, loss = 42.81274887919426\n",
      "Epoch = 118, loss = 42.68817958235741\n",
      "Epoch = 119, loss = 42.530912697315216\n",
      "Epoch = 120, loss = 42.658072739839554\n",
      "j = 321, Train Acc = 0.9131493506493507, Test Acc = 0.9078947368421053\n",
      "j = 250, Train Acc = 0.7086038961038961, Test Acc = 0.6973684210526315\n",
      "j = 200, Train Acc = 0.6501623376623377, Test Acc = 0.6414473684210527\n"
     ]
    }
   ],
   "source": [
    "### training parameters\n",
    "\n",
    "insize=X_train.size(2)\n",
    "hsize=64\n",
    "outsize=2    #binary classification\n",
    "model = matchRNN(insize,hsize,outsize)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size=16\n",
    "num_batches = int(X_train.size(0) / batch_size)\n",
    "\n",
    "# train iterations\n",
    "for epoch in range(121):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 121, loss = 42.51482406258583\n",
      "Epoch = 122, loss = 42.443438082933426\n",
      "Epoch = 123, loss = 42.35908907651901\n",
      "Epoch = 124, loss = 42.339952290058136\n",
      "Epoch = 125, loss = 42.58680856227875\n",
      "Epoch = 126, loss = 42.6234216094017\n",
      "Epoch = 127, loss = 42.37803840637207\n",
      "Epoch = 128, loss = 42.269956052303314\n",
      "Epoch = 129, loss = 42.18683221936226\n",
      "Epoch = 130, loss = 42.08628311753273\n",
      "j = 321, Train Acc = 0.9147727272727273, Test Acc = 0.9144736842105263\n",
      "j = 250, Train Acc = 0.711038961038961, Test Acc = 0.7236842105263158\n",
      "j = 200, Train Acc = 0.6607142857142857, Test Acc = 0.6578947368421053\n",
      "Epoch = 131, loss = 42.05293822288513\n",
      "Epoch = 132, loss = 42.00405728816986\n",
      "Epoch = 133, loss = 41.97373500466347\n",
      "Epoch = 134, loss = 41.890100330114365\n",
      "Epoch = 135, loss = 41.84604549407959\n",
      "Epoch = 136, loss = 41.764572978019714\n",
      "Epoch = 137, loss = 41.860697120428085\n",
      "Epoch = 138, loss = 41.77369737625122\n",
      "Epoch = 139, loss = 41.814272195100784\n",
      "Epoch = 140, loss = 41.709476947784424\n",
      "j = 321, Train Acc = 0.9180194805194806, Test Acc = 0.9111842105263158\n",
      "j = 250, Train Acc = 0.7305194805194806, Test Acc = 0.7039473684210527\n",
      "j = 200, Train Acc = 0.6696428571428571, Test Acc = 0.6414473684210527\n",
      "Epoch = 141, loss = 41.579577684402466\n",
      "Epoch = 142, loss = 41.58331888914108\n",
      "Epoch = 143, loss = 41.569507867097855\n",
      "Epoch = 144, loss = 41.62630054354668\n",
      "Epoch = 145, loss = 41.813482880592346\n",
      "Epoch = 146, loss = 41.42387220263481\n",
      "Epoch = 147, loss = 41.4316962659359\n",
      "Epoch = 148, loss = 42.07202157378197\n",
      "Epoch = 149, loss = 42.510345071554184\n",
      "Epoch = 150, loss = 41.90707719326019\n",
      "j = 321, Train Acc = 0.9204545454545454, Test Acc = 0.9111842105263158\n",
      "j = 250, Train Acc = 0.7272727272727273, Test Acc = 0.7072368421052632\n",
      "j = 200, Train Acc = 0.6818181818181818, Test Acc = 0.631578947368421\n",
      "Epoch = 151, loss = 41.51179897785187\n",
      "Epoch = 152, loss = 41.40377390384674\n",
      "Epoch = 153, loss = 41.32775256037712\n",
      "Epoch = 154, loss = 41.268345296382904\n",
      "Epoch = 155, loss = 41.04962354898453\n",
      "Epoch = 156, loss = 41.33874970674515\n",
      "Epoch = 157, loss = 40.91080805659294\n",
      "Epoch = 158, loss = 40.693069368600845\n",
      "Epoch = 159, loss = 40.809791564941406\n",
      "Epoch = 160, loss = 40.98497414588928\n",
      "j = 321, Train Acc = 0.9188311688311688, Test Acc = 0.9111842105263158\n",
      "j = 250, Train Acc = 0.7362012987012987, Test Acc = 0.7138157894736842\n",
      "j = 200, Train Acc = 0.6818181818181818, Test Acc = 0.6381578947368421\n",
      "Epoch = 161, loss = 41.48491045832634\n",
      "Epoch = 162, loss = 40.654208183288574\n",
      "Epoch = 163, loss = 40.5134414434433\n",
      "Epoch = 164, loss = 40.71220216155052\n",
      "Epoch = 165, loss = 40.61112251877785\n",
      "Epoch = 166, loss = 44.9981295466423\n",
      "Epoch = 167, loss = 40.96641990542412\n",
      "Epoch = 168, loss = 40.53809881210327\n",
      "Epoch = 169, loss = 39.894516736269\n",
      "Epoch = 170, loss = 40.9824033677578\n",
      "j = 321, Train Acc = 0.9326298701298701, Test Acc = 0.9111842105263158\n",
      "j = 250, Train Acc = 0.7297077922077922, Test Acc = 0.694078947368421\n",
      "j = 200, Train Acc = 0.6948051948051948, Test Acc = 0.6151315789473685\n",
      "Epoch = 171, loss = 40.0416354238987\n",
      "Epoch = 172, loss = 39.89895361661911\n",
      "Epoch = 173, loss = 40.36758241057396\n",
      "Epoch = 174, loss = 40.98101934790611\n",
      "Epoch = 175, loss = 40.644740641117096\n",
      "Epoch = 176, loss = 40.56793686747551\n",
      "Epoch = 177, loss = 40.46635124087334\n",
      "Epoch = 178, loss = 39.64746758341789\n",
      "Epoch = 179, loss = 40.13870370388031\n",
      "Epoch = 180, loss = 39.55544322729111\n",
      "j = 321, Train Acc = 0.935064935064935, Test Acc = 0.8980263157894737\n",
      "j = 250, Train Acc = 0.7597402597402597, Test Acc = 0.6907894736842105\n",
      "j = 200, Train Acc = 0.7126623376623377, Test Acc = 0.6052631578947368\n",
      "Epoch = 181, loss = 40.57871291041374\n",
      "Epoch = 182, loss = 39.93279352784157\n",
      "Epoch = 183, loss = 39.73638200759888\n",
      "Epoch = 184, loss = 39.00528672337532\n",
      "Epoch = 185, loss = 39.42239823937416\n",
      "Epoch = 186, loss = 39.12457114458084\n",
      "Epoch = 187, loss = 38.702237755060196\n",
      "Epoch = 188, loss = 38.84686863422394\n",
      "Epoch = 189, loss = 39.279219418764114\n",
      "Epoch = 190, loss = 39.30040529370308\n",
      "j = 321, Train Acc = 0.9334415584415584, Test Acc = 0.9144736842105263\n",
      "j = 250, Train Acc = 0.7597402597402597, Test Acc = 0.6907894736842105\n",
      "j = 200, Train Acc = 0.7183441558441559, Test Acc = 0.618421052631579\n",
      "Epoch = 191, loss = 38.41121780872345\n",
      "Epoch = 192, loss = 37.867923855781555\n",
      "Epoch = 193, loss = 37.30653312802315\n",
      "Epoch = 194, loss = 38.28773185610771\n",
      "Epoch = 195, loss = 37.09237930178642\n",
      "Epoch = 196, loss = 37.93026125431061\n",
      "Epoch = 197, loss = 37.46550387144089\n",
      "Epoch = 198, loss = 37.04900726675987\n",
      "Epoch = 199, loss = 36.93875378370285\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(121,200):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results on multi output lstm\n",
    "\n",
    "# Epoch = 80, loss = 42.69335952401161\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9473684210526315\n",
    "# j = 250, Train Acc = 0.7021103896103896, Test Acc = 0.7401315789473685\n",
    "# j = 200, Train Acc = 0.6323051948051948, Test Acc = 0.6381578947368421\n",
    "# Epoch = 81, loss = 42.64999434351921\n",
    "# Epoch = 82, loss = 42.63620883226395\n",
    "# Epoch = 83, loss = 42.57252901792526\n",
    "# Epoch = 84, loss = 42.54386180639267\n",
    "# Epoch = 85, loss = 42.543820798397064\n",
    "# Epoch = 86, loss = 42.522236466407776\n",
    "# Epoch = 87, loss = 43.12302175164223\n",
    "# Epoch = 88, loss = 43.80488169193268\n",
    "# Epoch = 89, loss = 43.530466586351395\n",
    "# Epoch = 90, loss = 43.1253487765789\n",
    "# j = 321, Train Acc = 0.9042207792207793, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.6996753246753247, Test Acc = 0.7467105263157895\n",
    "# j = 200, Train Acc = 0.635551948051948, Test Acc = 0.6644736842105263\n",
    "# Epoch = 91, loss = 42.85170575976372\n",
    "# Epoch = 92, loss = 42.611751973629\n",
    "# Epoch = 93, loss = 42.60165584087372\n",
    "# Epoch = 94, loss = 42.472452610731125\n",
    "# Epoch = 95, loss = 42.44126981496811\n",
    "# Epoch = 96, loss = 42.43537795543671\n",
    "# Epoch = 97, loss = 42.41048192977905\n",
    "# Epoch = 98, loss = 42.40796732902527\n",
    "# Epoch = 99, loss = 42.37926670908928\n",
    "# Epoch = 100, loss = 42.36977231502533\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9572368421052632\n",
    "# j = 250, Train Acc = 0.7021103896103896, Test Acc = 0.743421052631579\n",
    "# j = 200, Train Acc = 0.635551948051948, Test Acc = 0.6578947368421053\n",
    "# Epoch = 101, loss = 42.35787692666054\n",
    "# Epoch = 102, loss = 42.33479583263397\n",
    "# Epoch = 103, loss = 42.33716815710068\n",
    "# Epoch = 104, loss = 42.291197776794434\n",
    "# Epoch = 105, loss = 42.42200693488121\n",
    "# Epoch = 106, loss = 42.384660959243774\n",
    "# Epoch = 107, loss = 42.503537118434906\n",
    "# Epoch = 108, loss = 42.54160389304161\n",
    "# Epoch = 109, loss = 42.657554507255554\n",
    "# Epoch = 110, loss = 46.20248129963875\n",
    "# j = 321, Train Acc = 0.5048701298701299, Test Acc = 0.5526315789473685\n",
    "# j = 250, Train Acc = 0.5876623376623377, Test Acc = 0.6052631578947368\n",
    "# j = 200, Train Acc = 0.5909090909090909, Test Acc = 0.5953947368421053\n",
    "# Epoch = 111, loss = 49.134992361068726\n",
    "# Epoch = 112, loss = 43.54741933941841\n",
    "# Epoch = 113, loss = 42.86708441376686\n",
    "# Epoch = 114, loss = 42.59751954674721\n",
    "# Epoch = 115, loss = 42.5811333656311\n",
    "# Epoch = 116, loss = 42.51113286614418\n",
    "# Epoch = 117, loss = 42.68782064318657\n",
    "# Epoch = 118, loss = 42.618944466114044\n",
    "# Epoch = 119, loss = 42.34784010052681\n",
    "# Epoch = 120, loss = 42.301506608724594\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.713474025974026, Test Acc = 0.75\n",
    "# j = 200, Train Acc = 0.650974025974026, Test Acc = 0.6710526315789473\n",
    "# Epoch = 121, loss = 42.29225406050682\n",
    "# Epoch = 122, loss = 42.695088654756546\n",
    "# Epoch = 123, loss = 42.92030057311058\n",
    "# Epoch = 124, loss = 42.58220049738884\n",
    "# Epoch = 125, loss = 42.21531546115875\n",
    "# Epoch = 126, loss = 42.28876554965973\n",
    "# Epoch = 127, loss = 42.07214707136154\n",
    "# Epoch = 128, loss = 42.15156552195549\n",
    "# Epoch = 129, loss = 42.09659793972969\n",
    "# Epoch = 130, loss = 43.25178563594818\n",
    "# j = 321, Train Acc = 0.8522727272727273, Test Acc = 0.8322368421052632\n",
    "# j = 250, Train Acc = 0.6566558441558441, Test Acc = 0.6414473684210527\n",
    "# j = 200, Train Acc = 0.6112012987012987, Test Acc = 0.5822368421052632\n",
    "# Epoch = 131, loss = 46.67593550682068\n",
    "# Epoch = 132, loss = 43.80374363064766\n",
    "# Epoch = 133, loss = 42.56290856003761\n",
    "# Epoch = 134, loss = 42.68502974510193\n",
    "# Epoch = 135, loss = 42.1684812605381\n",
    "# Epoch = 136, loss = 42.16793215274811\n",
    "# Epoch = 137, loss = 42.16465583443642\n",
    "# Epoch = 138, loss = 42.15301898121834\n",
    "# Epoch = 139, loss = 41.93092507123947\n",
    "# Epoch = 140, loss = 42.10533806681633\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7167207792207793, Test Acc = 0.75\n",
    "# j = 200, Train Acc = 0.6550324675324676, Test Acc = 0.6743421052631579\n",
    "# Epoch = 141, loss = 41.746142119169235\n",
    "# Epoch = 142, loss = 42.21582242846489\n",
    "# Epoch = 143, loss = 41.93819710612297\n",
    "# Epoch = 144, loss = 41.87680941820145\n",
    "# Epoch = 145, loss = 42.27157709002495\n",
    "# Epoch = 146, loss = 41.94843155145645\n",
    "# Epoch = 147, loss = 41.99655598402023\n",
    "# Epoch = 148, loss = 42.1709089577198\n",
    "# Epoch = 149, loss = 42.353795766830444\n",
    "# Epoch = 150, loss = 42.91742631793022\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7126623376623377, Test Acc = 0.7105263157894737\n",
    "# j = 200, Train Acc = 0.6501623376623377, Test Acc = 0.6217105263157895\n",
    "# Epoch = 151, loss = 42.41686064004898\n",
    "# Epoch = 152, loss = 42.26151829957962\n",
    "# Epoch = 153, loss = 42.90386986732483\n",
    "# Epoch = 154, loss = 42.91596955060959\n",
    "# Epoch = 155, loss = 42.06308516860008\n",
    "# Epoch = 156, loss = 42.01913532614708\n",
    "# Epoch = 157, loss = 41.886956721544266\n",
    "# Epoch = 158, loss = 42.28912091255188\n",
    "# Epoch = 159, loss = 41.89504021406174\n",
    "# Epoch = 160, loss = 41.59468686580658\n",
    "# j = 321, Train Acc = 0.9058441558441559, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7224025974025974, Test Acc = 0.7368421052631579\n",
    "# j = 200, Train Acc = 0.6574675324675324, Test Acc = 0.6447368421052632\n",
    "# Epoch = 161, loss = 42.02638882398605\n",
    "# Epoch = 162, loss = 42.76215770840645\n",
    "# Epoch = 163, loss = 41.943300515413284\n",
    "# Epoch = 164, loss = 41.744627594947815\n",
    "# Epoch = 165, loss = 41.64369750022888\n",
    "# Epoch = 166, loss = 41.82700064778328\n",
    "# Epoch = 167, loss = 41.838323920965195\n",
    "# Epoch = 168, loss = 42.99936231970787\n",
    "# Epoch = 169, loss = 42.55475810170174\n",
    "# Epoch = 170, loss = 42.43961876630783\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.724025974025974, Test Acc = 0.7236842105263158\n",
    "# j = 200, Train Acc = 0.6525974025974026, Test Acc = 0.6513157894736842\n",
    "# Epoch = 171, loss = 42.14726781845093\n",
    "# Epoch = 172, loss = 42.02729895710945\n",
    "# Epoch = 173, loss = 42.05796667933464\n",
    "# Epoch = 174, loss = 42.047207325696945\n",
    "# Epoch = 175, loss = 41.85863038897514\n",
    "# Epoch = 176, loss = 41.78750139474869\n",
    "# Epoch = 177, loss = 41.89787817001343\n",
    "# Epoch = 178, loss = 41.723336696624756\n",
    "# Epoch = 179, loss = 41.46990704536438\n",
    "# Epoch = 180, loss = 41.31174489855766\n",
    "# j = 321, Train Acc = 0.9147727272727273, Test Acc = 0.9539473684210527\n",
    "# j = 250, Train Acc = 0.7386363636363636, Test Acc = 0.743421052631579\n",
    "# j = 200, Train Acc = 0.6728896103896104, Test Acc = 0.6546052631578947\n",
    "# Epoch = 181, loss = 41.147144973278046\n",
    "# Epoch = 182, loss = 41.04915153980255\n",
    "# Epoch = 183, loss = 41.02901268005371\n",
    "# Epoch = 184, loss = 41.16104966402054\n",
    "# Epoch = 185, loss = 41.052144914865494\n",
    "# Epoch = 186, loss = 40.75493836402893\n",
    "# Epoch = 187, loss = 40.8061888217926\n",
    "# Epoch = 188, loss = 41.45923164486885\n",
    "# Epoch = 189, loss = 40.893089562654495\n",
    "# Epoch = 190, loss = 40.89757114648819\n",
    "# j = 321, Train Acc = 0.9188311688311688, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.734577922077922, Test Acc = 0.7138157894736842\n",
    "# j = 200, Train Acc = 0.6728896103896104, Test Acc = 0.6447368421052632\n",
    "# Epoch = 191, loss = 40.91097801923752\n",
    "# Epoch = 192, loss = 40.533461928367615\n",
    "# Epoch = 193, loss = 40.0526128411293\n",
    "# Epoch = 194, loss = 40.267620861530304\n",
    "# Epoch = 195, loss = 40.43658027052879\n",
    "# Epoch = 196, loss = 39.8879688680172\n",
    "# Epoch = 197, loss = 40.29349622130394\n",
    "# Epoch = 198, loss = 39.77166989445686\n",
    "# Epoch = 199, loss = 40.42837768793106\n",
    "# Epoch = 200, loss = 39.85610529780388\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7564935064935064, Test Acc = 0.7105263157894737\n",
    "# j = 200, Train Acc = 0.698051948051948, Test Acc = 0.6381578947368421"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
