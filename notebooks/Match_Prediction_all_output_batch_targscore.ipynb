{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and process data\n",
    "path = r'odi_csv/'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "shuffle(all_files)\n",
    "innings2 = []\n",
    "targ_score = []\n",
    "target=[]\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename,usecols=[0])\n",
    "    skips = df.loc[ 'info' , : ].shape[0]\n",
    "\n",
    "    df = pd.read_csv(filename,nrows=skips,skiprows=1,header=None)\n",
    "    df = df.drop(columns=0).set_index(df.columns[1])\n",
    "    winteam=None\n",
    "    if 'winner' in df.index:\n",
    "        winteam = df.loc['winner',:].values[0]\n",
    "    \n",
    "    df = pd.read_csv(filename,skiprows=skips+1,header=None)\n",
    "    df2 = df[df.columns[[1,2,7,8]]].set_index(df.columns[1]).drop(index=1)\n",
    "    df1_ = df[df.columns[[1,2,7,8]]].set_index(df.columns[1])\n",
    "    df1 = df1_[df1_.index==1]\n",
    "\n",
    "    if df2.shape[0]>0:\n",
    "        innings2.append(df2)\n",
    "        targ_score.append(df1[df1.columns[[1,2]]].sum().sum())\n",
    "        i2team = df[df.columns[[1,3]]].set_index(df.columns[1]).drop(index=1).values[0,0]\n",
    "        if (i2team==winteam):\n",
    "            target.append(1)\n",
    "        else:\n",
    "            target.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1244, 321, 3])\n",
      "torch.Size([312, 321, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch tensor processing\n",
    "features=[]\n",
    "# create targets first\n",
    "for i in range(len(innings2)):\n",
    "    features.append(torch.tensor(innings2[i].values))\n",
    "\n",
    "\n",
    "# convert to fixed length sequence\n",
    "features = pad_sequence(features,batch_first=True, padding_value=-1)\n",
    "targets = torch.tensor(target)\n",
    "\n",
    "split = int(len(features) * 0.8)\n",
    "X_train = features[:split]\n",
    "X_test  = features[split:]\n",
    "y_train = targets[:split]\n",
    "y_test  = targets[split:]\n",
    "print(X_train.size())\n",
    "print(X_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1244, 321, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_score_train = np.transpose(targ_score[:split] * np.ones((1,X_train.shape[1],X_train.shape[0])))\n",
    "targ_score_test = np.transpose(targ_score[split:] * np.ones((1,X_test.shape[1],X_test.shape[0])))\n",
    "X_train = torch.cat((X_train.float(), torch.Tensor(targ_score_train)), 2)\n",
    "X_test = torch.cat((X_test.float(), torch.Tensor(targ_score_test)), 2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "class matchRNN(nn.Module):\n",
    "    def __init__(self,insize,hsize,outsize):\n",
    "        super(matchRNN,self).__init__()\n",
    "        \n",
    "        self.insize=insize\n",
    "        self.hsize=hsize\n",
    "        self.outsize = outsize\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=insize, hidden_size=hsize)\n",
    "        self.fc_out = nn.Linear(in_features=hsize, out_features=outsize)\n",
    "#         self.dropout = nn.Dropout(p=0.2, inplace=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,feat):\n",
    "#         feat = torch.tensor(feat[np.newaxis,:],dtype=torch.float32)\n",
    "        batch_size = feat.size(0)\n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hsize))\n",
    "        cell_state = torch.zeros((batch_size, self.hsize))\n",
    "        outputs = torch.empty((batch_size, feat.size(1), self.outsize))\n",
    "\n",
    "        for t in range(feat.size(1)):\n",
    "\n",
    "            # for the first time step (if input is different)\n",
    "            if t == 0:\n",
    "                hidden_state, cell_state = self.lstm_cell(feat[:,t,:].view(batch_size,-1).float(), (hidden_state, cell_state))\n",
    "                \n",
    "            # for the 2nd+ time step\n",
    "            else:\n",
    "                hidden_state, cell_state = self.lstm_cell(feat[:,t,:].view(batch_size,-1).float(), (hidden_state, cell_state))\n",
    "            \n",
    "#             dropouts = self.dropout(hidden_state)\n",
    "            out = self.fc_out(hidden_state)\n",
    "#             out = self.softmax(out)\n",
    "            outputs[:,t,:] = out\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1244, 321])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain_tiled = y_train.repeat(X_train.size(1),1).transpose(0,1)\n",
    "ytrain_tiled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalfunc(model,j):\n",
    "    model.eval()\n",
    "    # train\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    corr=0\n",
    "    num_batches = X.size(0) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        op = model(X[start_idx:end_idx])\n",
    "        req_op = op[:,j-1]\n",
    "        maxval,maxidx = torch.max(req_op,1)\n",
    "        corr+= np.sum((maxidx==y[start_idx:end_idx]).numpy())\n",
    "    total=num_batches*batch_size\n",
    "    train_acc = corr / total\n",
    "    \n",
    "    # test\n",
    "    X = X_test\n",
    "    y = y_test\n",
    "    corr=0\n",
    "    num_batches = X.size(0) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        op = model(X[start_idx:end_idx])\n",
    "        req_op = op[:,j-1]\n",
    "        maxval,maxidx = torch.max(req_op,1)\n",
    "        corr+= np.sum((maxidx==y[start_idx:end_idx]).numpy())\n",
    "    total=num_batches*batch_size\n",
    "    test_acc = corr / total\n",
    "    print('j = {}, Train Acc = {}, Test Acc = {}'.format(j,train_acc,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, loss = 51.37752652168274\n",
      "j = 321, Train Acc = 0.724025974025974, Test Acc = 0.7631578947368421\n",
      "j = 250, Train Acc = 0.7272727272727273, Test Acc = 0.756578947368421\n",
      "j = 200, Train Acc = 0.7215909090909091, Test Acc = 0.7664473684210527\n",
      "Epoch = 1, loss = 45.04612010717392\n",
      "Epoch = 2, loss = 44.36398956179619\n",
      "Epoch = 3, loss = 43.8653658926487\n",
      "Epoch = 4, loss = 43.48948773741722\n",
      "Epoch = 5, loss = 43.251995384693146\n",
      "Epoch = 6, loss = 43.06429895758629\n",
      "Epoch = 7, loss = 42.83167240023613\n",
      "Epoch = 8, loss = 42.69397082924843\n",
      "Epoch = 9, loss = 42.483425706624985\n",
      "Epoch = 10, loss = 42.30640250444412\n",
      "j = 321, Train Acc = 0.7329545454545454, Test Acc = 0.7664473684210527\n",
      "j = 250, Train Acc = 0.7402597402597403, Test Acc = 0.7730263157894737\n",
      "j = 200, Train Acc = 0.7426948051948052, Test Acc = 0.7763157894736842\n",
      "Epoch = 11, loss = 42.120668828487396\n",
      "Epoch = 12, loss = 41.97081384062767\n",
      "Epoch = 13, loss = 41.76253962516785\n",
      "Epoch = 14, loss = 41.61461901664734\n",
      "Epoch = 15, loss = 41.47462859749794\n",
      "Epoch = 16, loss = 41.345596224069595\n",
      "Epoch = 17, loss = 41.2310351729393\n",
      "Epoch = 18, loss = 41.13819667696953\n",
      "Epoch = 19, loss = 40.9843735396862\n",
      "Epoch = 20, loss = 40.886232405900955\n",
      "j = 321, Train Acc = 0.7410714285714286, Test Acc = 0.7598684210526315\n",
      "j = 250, Train Acc = 0.7491883116883117, Test Acc = 0.7894736842105263\n",
      "j = 200, Train Acc = 0.7532467532467533, Test Acc = 0.7763157894736842\n",
      "Epoch = 21, loss = 40.77959984540939\n",
      "Epoch = 22, loss = 40.67971360683441\n",
      "Epoch = 23, loss = 40.57879263162613\n",
      "Epoch = 24, loss = 40.573555290699005\n",
      "Epoch = 25, loss = 40.434330701828\n",
      "Epoch = 26, loss = 40.3497234582901\n",
      "Epoch = 27, loss = 40.28916981816292\n",
      "Epoch = 28, loss = 40.20080929994583\n",
      "Epoch = 29, loss = 40.04185143113136\n",
      "Epoch = 30, loss = 40.068268448114395\n",
      "j = 321, Train Acc = 0.7573051948051948, Test Acc = 0.7796052631578947\n",
      "j = 250, Train Acc = 0.7564935064935064, Test Acc = 0.7894736842105263\n",
      "j = 200, Train Acc = 0.7670454545454546, Test Acc = 0.7861842105263158\n",
      "Epoch = 31, loss = 39.972987562417984\n",
      "Epoch = 32, loss = 39.89351034164429\n",
      "Epoch = 33, loss = 39.979219287633896\n",
      "Epoch = 34, loss = 39.74440550804138\n",
      "Epoch = 35, loss = 39.647505432367325\n",
      "Epoch = 36, loss = 39.57589903473854\n",
      "Epoch = 37, loss = 39.53719288110733\n",
      "Epoch = 38, loss = 39.43976077437401\n",
      "Epoch = 39, loss = 39.375887244939804\n",
      "Epoch = 40, loss = 39.27687606215477\n",
      "j = 321, Train Acc = 0.7792207792207793, Test Acc = 0.7927631578947368\n",
      "j = 250, Train Acc = 0.760551948051948, Test Acc = 0.7960526315789473\n",
      "j = 200, Train Acc = 0.765422077922078, Test Acc = 0.7894736842105263\n",
      "Epoch = 41, loss = 39.05154472589493\n",
      "Epoch = 42, loss = 38.73991033434868\n",
      "Epoch = 43, loss = 38.455392211675644\n",
      "Epoch = 44, loss = 38.257100045681\n",
      "Epoch = 45, loss = 38.08080244064331\n",
      "Epoch = 46, loss = 37.73809650540352\n",
      "Epoch = 47, loss = 37.58920541405678\n",
      "Epoch = 48, loss = 37.53766757249832\n",
      "Epoch = 49, loss = 37.20922929048538\n",
      "Epoch = 50, loss = 36.80257806181908\n",
      "j = 321, Train Acc = 0.8246753246753247, Test Acc = 0.8453947368421053\n",
      "j = 250, Train Acc = 0.7832792207792207, Test Acc = 0.8026315789473685\n",
      "j = 200, Train Acc = 0.7808441558441559, Test Acc = 0.7960526315789473\n",
      "Epoch = 51, loss = 36.511814922094345\n",
      "Epoch = 52, loss = 36.315607607364655\n",
      "Epoch = 53, loss = 36.08470046520233\n",
      "Epoch = 54, loss = 35.650273114442825\n",
      "Epoch = 55, loss = 35.4631587266922\n",
      "Epoch = 56, loss = 35.56391963362694\n",
      "Epoch = 57, loss = 35.1459746658802\n",
      "Epoch = 58, loss = 34.86055698990822\n",
      "Epoch = 59, loss = 34.558862388134\n",
      "Epoch = 60, loss = 34.368249624967575\n",
      "j = 321, Train Acc = 0.8944805194805194, Test Acc = 0.8947368421052632\n",
      "j = 250, Train Acc = 0.8060064935064936, Test Acc = 0.805921052631579\n",
      "j = 200, Train Acc = 0.7832792207792207, Test Acc = 0.8125\n",
      "Epoch = 61, loss = 34.14769260585308\n",
      "Epoch = 62, loss = 33.94062712788582\n",
      "Epoch = 63, loss = 33.75997892022133\n",
      "Epoch = 64, loss = 33.80425110459328\n",
      "Epoch = 65, loss = 33.51215223968029\n",
      "Epoch = 66, loss = 33.28032706677914\n",
      "Epoch = 67, loss = 33.220742136240005\n",
      "Epoch = 68, loss = 33.013037502765656\n",
      "Epoch = 69, loss = 32.82510247826576\n",
      "Epoch = 70, loss = 32.68978953361511\n",
      "j = 321, Train Acc = 0.9155844155844156, Test Acc = 0.9177631578947368\n",
      "j = 250, Train Acc = 0.814935064935065, Test Acc = 0.8388157894736842\n",
      "j = 200, Train Acc = 0.7840909090909091, Test Acc = 0.8355263157894737\n",
      "Epoch = 71, loss = 32.57212224602699\n",
      "Epoch = 72, loss = 32.509102150797844\n",
      "Epoch = 73, loss = 32.40421259403229\n",
      "Epoch = 74, loss = 32.308572232723236\n",
      "Epoch = 75, loss = 32.11202149093151\n",
      "Epoch = 76, loss = 32.09739442169666\n",
      "Epoch = 77, loss = 31.833692252635956\n",
      "Epoch = 78, loss = 31.8630231320858\n",
      "Epoch = 79, loss = 31.781061872839928\n",
      "Epoch = 80, loss = 31.55401074886322\n",
      "j = 321, Train Acc = 0.9342532467532467, Test Acc = 0.9210526315789473\n",
      "j = 250, Train Acc = 0.823051948051948, Test Acc = 0.8585526315789473\n",
      "j = 200, Train Acc = 0.7962662337662337, Test Acc = 0.8486842105263158\n",
      "Epoch = 81, loss = 31.506531715393066\n",
      "Epoch = 82, loss = 31.33991903066635\n",
      "Epoch = 83, loss = 31.189647391438484\n",
      "Epoch = 84, loss = 31.087447091937065\n",
      "Epoch = 85, loss = 31.01749439537525\n",
      "Epoch = 86, loss = 31.062593445181847\n",
      "Epoch = 87, loss = 31.102680504322052\n",
      "Epoch = 88, loss = 31.12495169043541\n",
      "Epoch = 89, loss = 32.00259093940258\n",
      "Epoch = 90, loss = 31.25358560681343\n",
      "j = 321, Train Acc = 0.9456168831168831, Test Acc = 0.9375\n",
      "j = 250, Train Acc = 0.8262987012987013, Test Acc = 0.8585526315789473\n",
      "j = 200, Train Acc = 0.7978896103896104, Test Acc = 0.8355263157894737\n",
      "Epoch = 91, loss = 30.715047046542168\n",
      "Epoch = 92, loss = 30.62156556546688\n",
      "Epoch = 93, loss = 30.569091260433197\n",
      "Epoch = 94, loss = 30.53395803272724\n",
      "Epoch = 95, loss = 30.464232489466667\n",
      "Epoch = 96, loss = 30.42499005794525\n",
      "Epoch = 97, loss = 30.39422270655632\n",
      "Epoch = 98, loss = 30.198761105537415\n",
      "Epoch = 99, loss = 30.132715120911598\n",
      "Epoch = 100, loss = 30.061398461461067\n",
      "j = 321, Train Acc = 0.9512987012987013, Test Acc = 0.9407894736842105\n",
      "j = 250, Train Acc = 0.8498376623376623, Test Acc = 0.881578947368421\n",
      "j = 200, Train Acc = 0.8198051948051948, Test Acc = 0.8782894736842105\n",
      "Epoch = 101, loss = 29.999843567609787\n",
      "Epoch = 102, loss = 29.94388337433338\n",
      "Epoch = 103, loss = 29.889538556337357\n",
      "Epoch = 104, loss = 29.843486487865448\n",
      "Epoch = 105, loss = 29.807464063167572\n",
      "Epoch = 106, loss = 30.25379154086113\n",
      "Epoch = 107, loss = 29.973092690110207\n",
      "Epoch = 108, loss = 29.73401266336441\n",
      "Epoch = 109, loss = 29.72069564461708\n",
      "Epoch = 110, loss = 29.71388953924179\n",
      "j = 321, Train Acc = 0.950487012987013, Test Acc = 0.9407894736842105\n",
      "j = 250, Train Acc = 0.8482142857142857, Test Acc = 0.8881578947368421\n",
      "j = 200, Train Acc = 0.8246753246753247, Test Acc = 0.8782894736842105\n",
      "Epoch = 111, loss = 29.643885612487793\n",
      "Epoch = 112, loss = 29.682733342051506\n",
      "Epoch = 113, loss = 29.564177721738815\n",
      "Epoch = 114, loss = 29.602591425180435\n",
      "Epoch = 115, loss = 29.496157437562943\n",
      "Epoch = 116, loss = 29.500430911779404\n",
      "Epoch = 117, loss = 29.449808940291405\n",
      "Epoch = 118, loss = 29.45739781856537\n",
      "Epoch = 119, loss = 29.43581920862198\n",
      "Epoch = 120, loss = 29.384156569838524\n",
      "j = 321, Train Acc = 0.9496753246753247, Test Acc = 0.9375\n",
      "j = 250, Train Acc = 0.8563311688311688, Test Acc = 0.8980263157894737\n",
      "j = 200, Train Acc = 0.8246753246753247, Test Acc = 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "### training parameters\n",
    "\n",
    "insize=X_train.size(2)\n",
    "hsize=64\n",
    "outsize=2    #binary classification\n",
    "model = matchRNN(insize,hsize,outsize)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size=16\n",
    "num_batches = int(X_train.size(0) / batch_size)\n",
    "\n",
    "# train iterations\n",
    "for epoch in range(121):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch_targscore.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 121, loss = 29.36156725883484\n",
      "Epoch = 122, loss = 29.291331201791763\n",
      "Epoch = 123, loss = 29.289401590824127\n",
      "Epoch = 124, loss = 29.215451017022133\n",
      "Epoch = 125, loss = 29.22421135008335\n",
      "Epoch = 126, loss = 29.1416534781456\n",
      "Epoch = 127, loss = 29.160805478692055\n",
      "Epoch = 128, loss = 29.075149193406105\n",
      "Epoch = 129, loss = 29.104718685150146\n",
      "Epoch = 130, loss = 29.1018797904253\n",
      "j = 321, Train Acc = 0.952922077922078, Test Acc = 0.9407894736842105\n",
      "j = 250, Train Acc = 0.859577922077922, Test Acc = 0.9013157894736842\n",
      "j = 200, Train Acc = 0.8295454545454546, Test Acc = 0.8717105263157895\n",
      "Epoch = 131, loss = 29.09407015144825\n",
      "Epoch = 132, loss = 29.00515966117382\n",
      "Epoch = 133, loss = 28.94416408240795\n",
      "Epoch = 134, loss = 28.92048989236355\n",
      "Epoch = 135, loss = 28.957733616232872\n",
      "Epoch = 136, loss = 28.975262477993965\n",
      "Epoch = 137, loss = 28.782202199101448\n",
      "Epoch = 138, loss = 28.802495658397675\n",
      "Epoch = 139, loss = 28.84838931262493\n",
      "Epoch = 140, loss = 28.840678960084915\n",
      "j = 321, Train Acc = 0.952922077922078, Test Acc = 0.9473684210526315\n",
      "j = 250, Train Acc = 0.859577922077922, Test Acc = 0.8980263157894737\n",
      "j = 200, Train Acc = 0.8336038961038961, Test Acc = 0.8717105263157895\n",
      "Epoch = 141, loss = 28.690621972084045\n",
      "Epoch = 142, loss = 28.733671203255653\n",
      "Epoch = 143, loss = 28.736706733703613\n",
      "Epoch = 144, loss = 28.590274140238762\n",
      "Epoch = 145, loss = 28.981285840272903\n",
      "Epoch = 146, loss = 28.43548096716404\n",
      "Epoch = 147, loss = 28.593174263834953\n",
      "Epoch = 148, loss = 28.60152977705002\n",
      "Epoch = 149, loss = 28.62782445549965\n",
      "Epoch = 150, loss = 28.48904873430729\n",
      "j = 321, Train Acc = 0.9545454545454546, Test Acc = 0.9539473684210527\n",
      "j = 250, Train Acc = 0.8668831168831169, Test Acc = 0.8980263157894737\n",
      "j = 200, Train Acc = 0.8368506493506493, Test Acc = 0.875\n",
      "Epoch = 151, loss = 28.726407811045647\n",
      "Epoch = 152, loss = 28.455029860138893\n",
      "Epoch = 153, loss = 28.57364846765995\n",
      "Epoch = 154, loss = 28.337787091732025\n",
      "Epoch = 155, loss = 28.75029468536377\n",
      "Epoch = 156, loss = 28.58200341463089\n",
      "Epoch = 157, loss = 28.375462919473648\n",
      "Epoch = 158, loss = 28.445526346564293\n",
      "Epoch = 159, loss = 28.400375112891197\n",
      "Epoch = 160, loss = 28.358241111040115\n",
      "j = 321, Train Acc = 0.9553571428571429, Test Acc = 0.9506578947368421\n",
      "j = 250, Train Acc = 0.8652597402597403, Test Acc = 0.8980263157894737\n",
      "j = 200, Train Acc = 0.8449675324675324, Test Acc = 0.8651315789473685\n",
      "Epoch = 161, loss = 28.484114170074463\n",
      "Epoch = 162, loss = 28.406367525458336\n",
      "Epoch = 163, loss = 28.32558350265026\n",
      "Epoch = 164, loss = 28.32994845509529\n",
      "Epoch = 165, loss = 28.218023538589478\n",
      "Epoch = 166, loss = 28.14529164135456\n",
      "Epoch = 167, loss = 28.285272285342216\n",
      "Epoch = 168, loss = 28.025586411356926\n",
      "Epoch = 169, loss = 28.35651622712612\n",
      "Epoch = 170, loss = 28.183138713240623\n",
      "j = 321, Train Acc = 0.9569805194805194, Test Acc = 0.9506578947368421\n",
      "j = 250, Train Acc = 0.8758116883116883, Test Acc = 0.9013157894736842\n",
      "j = 200, Train Acc = 0.8530844155844156, Test Acc = 0.868421052631579\n",
      "Epoch = 171, loss = 28.24890837073326\n",
      "Epoch = 172, loss = 28.164773538708687\n",
      "Epoch = 173, loss = 28.292969167232513\n",
      "Epoch = 174, loss = 27.97082817554474\n",
      "Epoch = 175, loss = 28.14909227192402\n",
      "Epoch = 176, loss = 28.017631351947784\n",
      "Epoch = 177, loss = 28.062787041068077\n",
      "Epoch = 178, loss = 28.130045488476753\n",
      "Epoch = 179, loss = 28.041981115937233\n",
      "Epoch = 180, loss = 28.020889028906822\n",
      "j = 321, Train Acc = 0.9553571428571429, Test Acc = 0.9407894736842105\n",
      "j = 250, Train Acc = 0.872564935064935, Test Acc = 0.8914473684210527\n",
      "j = 200, Train Acc = 0.851461038961039, Test Acc = 0.8618421052631579\n",
      "Epoch = 181, loss = 27.968574464321136\n",
      "Epoch = 182, loss = 27.997133314609528\n",
      "Epoch = 183, loss = 27.931515261530876\n",
      "Epoch = 184, loss = 27.94770911335945\n",
      "Epoch = 185, loss = 27.819688752293587\n",
      "Epoch = 186, loss = 28.057323187589645\n",
      "Epoch = 187, loss = 27.89424705505371\n",
      "Epoch = 188, loss = 27.814007088541985\n",
      "Epoch = 189, loss = 27.81748804450035\n",
      "Epoch = 190, loss = 27.73754645884037\n",
      "j = 321, Train Acc = 0.9553571428571429, Test Acc = 0.9407894736842105\n",
      "j = 250, Train Acc = 0.872564935064935, Test Acc = 0.9013157894736842\n",
      "j = 200, Train Acc = 0.8612012987012987, Test Acc = 0.8618421052631579\n",
      "Epoch = 191, loss = 27.64379481226206\n",
      "Epoch = 192, loss = 27.903538569808006\n",
      "Epoch = 193, loss = 27.66806349158287\n",
      "Epoch = 194, loss = 27.609157621860504\n",
      "Epoch = 195, loss = 27.902345791459084\n",
      "Epoch = 196, loss = 27.64264675974846\n",
      "Epoch = 197, loss = 27.57602210342884\n",
      "Epoch = 198, loss = 27.678369522094727\n",
      "Epoch = 199, loss = 28.490989595651627\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(121,201):  # optimum 100-150 epochs\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        model.zero_grad()\n",
    "        outputs = model(X_train[start_idx:end_idx])\n",
    "        loss = loss_function(outputs[:,60:321,:].contiguous().view(-1,outsize), ytrain_tiled[start_idx:end_idx,60:321].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss+=loss.data.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch = {}, loss = {}'.format(epoch,epoch_loss))\n",
    "    if epoch%10==0:\n",
    "        for j in [321,250,200]:\n",
    "            evalfunc(model,j)\n",
    "\n",
    "torch.save(model.state_dict(), './models/cric_prediction_all_output_batch_targscore.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j = 321, Train Acc = 0.9553571428571429, Test Acc = 0.9539473684210527\n",
      "j = 250, Train Acc = 0.8676948051948052, Test Acc = 0.8980263157894737\n",
      "j = 200, Train Acc = 0.851461038961039, Test Acc = 0.8618421052631579\n",
      "j = 150, Train Acc = 0.8076298701298701, Test Acc = 0.8519736842105263\n",
      "j = 100, Train Acc = 0.775974025974026, Test Acc = 0.8223684210526315\n",
      "j = 60, Train Acc = 0.7564935064935064, Test Acc = 0.7894736842105263\n",
      "j = 6, Train Acc = 0.6347402597402597, Test Acc = 0.6644736842105263\n"
     ]
    }
   ],
   "source": [
    "for j in [321,250,200,150,100,60,6]:\n",
    "        evalfunc(model,j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j = 1, Train Acc = 0.5016233766233766, Test Acc = 0.5032894736842105\n"
     ]
    }
   ],
   "source": [
    "evalfunc(model,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results on multi output lstm\n",
    "\n",
    "# Epoch = 80, loss = 42.69335952401161\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9473684210526315\n",
    "# j = 250, Train Acc = 0.7021103896103896, Test Acc = 0.7401315789473685\n",
    "# j = 200, Train Acc = 0.6323051948051948, Test Acc = 0.6381578947368421\n",
    "# Epoch = 81, loss = 42.64999434351921\n",
    "# Epoch = 82, loss = 42.63620883226395\n",
    "# Epoch = 83, loss = 42.57252901792526\n",
    "# Epoch = 84, loss = 42.54386180639267\n",
    "# Epoch = 85, loss = 42.543820798397064\n",
    "# Epoch = 86, loss = 42.522236466407776\n",
    "# Epoch = 87, loss = 43.12302175164223\n",
    "# Epoch = 88, loss = 43.80488169193268\n",
    "# Epoch = 89, loss = 43.530466586351395\n",
    "# Epoch = 90, loss = 43.1253487765789\n",
    "# j = 321, Train Acc = 0.9042207792207793, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.6996753246753247, Test Acc = 0.7467105263157895\n",
    "# j = 200, Train Acc = 0.635551948051948, Test Acc = 0.6644736842105263\n",
    "# Epoch = 91, loss = 42.85170575976372\n",
    "# Epoch = 92, loss = 42.611751973629\n",
    "# Epoch = 93, loss = 42.60165584087372\n",
    "# Epoch = 94, loss = 42.472452610731125\n",
    "# Epoch = 95, loss = 42.44126981496811\n",
    "# Epoch = 96, loss = 42.43537795543671\n",
    "# Epoch = 97, loss = 42.41048192977905\n",
    "# Epoch = 98, loss = 42.40796732902527\n",
    "# Epoch = 99, loss = 42.37926670908928\n",
    "# Epoch = 100, loss = 42.36977231502533\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9572368421052632\n",
    "# j = 250, Train Acc = 0.7021103896103896, Test Acc = 0.743421052631579\n",
    "# j = 200, Train Acc = 0.635551948051948, Test Acc = 0.6578947368421053\n",
    "# Epoch = 101, loss = 42.35787692666054\n",
    "# Epoch = 102, loss = 42.33479583263397\n",
    "# Epoch = 103, loss = 42.33716815710068\n",
    "# Epoch = 104, loss = 42.291197776794434\n",
    "# Epoch = 105, loss = 42.42200693488121\n",
    "# Epoch = 106, loss = 42.384660959243774\n",
    "# Epoch = 107, loss = 42.503537118434906\n",
    "# Epoch = 108, loss = 42.54160389304161\n",
    "# Epoch = 109, loss = 42.657554507255554\n",
    "# Epoch = 110, loss = 46.20248129963875\n",
    "# j = 321, Train Acc = 0.5048701298701299, Test Acc = 0.5526315789473685\n",
    "# j = 250, Train Acc = 0.5876623376623377, Test Acc = 0.6052631578947368\n",
    "# j = 200, Train Acc = 0.5909090909090909, Test Acc = 0.5953947368421053\n",
    "# Epoch = 111, loss = 49.134992361068726\n",
    "# Epoch = 112, loss = 43.54741933941841\n",
    "# Epoch = 113, loss = 42.86708441376686\n",
    "# Epoch = 114, loss = 42.59751954674721\n",
    "# Epoch = 115, loss = 42.5811333656311\n",
    "# Epoch = 116, loss = 42.51113286614418\n",
    "# Epoch = 117, loss = 42.68782064318657\n",
    "# Epoch = 118, loss = 42.618944466114044\n",
    "# Epoch = 119, loss = 42.34784010052681\n",
    "# Epoch = 120, loss = 42.301506608724594\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.713474025974026, Test Acc = 0.75\n",
    "# j = 200, Train Acc = 0.650974025974026, Test Acc = 0.6710526315789473\n",
    "# Epoch = 121, loss = 42.29225406050682\n",
    "# Epoch = 122, loss = 42.695088654756546\n",
    "# Epoch = 123, loss = 42.92030057311058\n",
    "# Epoch = 124, loss = 42.58220049738884\n",
    "# Epoch = 125, loss = 42.21531546115875\n",
    "# Epoch = 126, loss = 42.28876554965973\n",
    "# Epoch = 127, loss = 42.07214707136154\n",
    "# Epoch = 128, loss = 42.15156552195549\n",
    "# Epoch = 129, loss = 42.09659793972969\n",
    "# Epoch = 130, loss = 43.25178563594818\n",
    "# j = 321, Train Acc = 0.8522727272727273, Test Acc = 0.8322368421052632\n",
    "# j = 250, Train Acc = 0.6566558441558441, Test Acc = 0.6414473684210527\n",
    "# j = 200, Train Acc = 0.6112012987012987, Test Acc = 0.5822368421052632\n",
    "# Epoch = 131, loss = 46.67593550682068\n",
    "# Epoch = 132, loss = 43.80374363064766\n",
    "# Epoch = 133, loss = 42.56290856003761\n",
    "# Epoch = 134, loss = 42.68502974510193\n",
    "# Epoch = 135, loss = 42.1684812605381\n",
    "# Epoch = 136, loss = 42.16793215274811\n",
    "# Epoch = 137, loss = 42.16465583443642\n",
    "# Epoch = 138, loss = 42.15301898121834\n",
    "# Epoch = 139, loss = 41.93092507123947\n",
    "# Epoch = 140, loss = 42.10533806681633\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7167207792207793, Test Acc = 0.75\n",
    "# j = 200, Train Acc = 0.6550324675324676, Test Acc = 0.6743421052631579\n",
    "# Epoch = 141, loss = 41.746142119169235\n",
    "# Epoch = 142, loss = 42.21582242846489\n",
    "# Epoch = 143, loss = 41.93819710612297\n",
    "# Epoch = 144, loss = 41.87680941820145\n",
    "# Epoch = 145, loss = 42.27157709002495\n",
    "# Epoch = 146, loss = 41.94843155145645\n",
    "# Epoch = 147, loss = 41.99655598402023\n",
    "# Epoch = 148, loss = 42.1709089577198\n",
    "# Epoch = 149, loss = 42.353795766830444\n",
    "# Epoch = 150, loss = 42.91742631793022\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7126623376623377, Test Acc = 0.7105263157894737\n",
    "# j = 200, Train Acc = 0.6501623376623377, Test Acc = 0.6217105263157895\n",
    "# Epoch = 151, loss = 42.41686064004898\n",
    "# Epoch = 152, loss = 42.26151829957962\n",
    "# Epoch = 153, loss = 42.90386986732483\n",
    "# Epoch = 154, loss = 42.91596955060959\n",
    "# Epoch = 155, loss = 42.06308516860008\n",
    "# Epoch = 156, loss = 42.01913532614708\n",
    "# Epoch = 157, loss = 41.886956721544266\n",
    "# Epoch = 158, loss = 42.28912091255188\n",
    "# Epoch = 159, loss = 41.89504021406174\n",
    "# Epoch = 160, loss = 41.59468686580658\n",
    "# j = 321, Train Acc = 0.9058441558441559, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7224025974025974, Test Acc = 0.7368421052631579\n",
    "# j = 200, Train Acc = 0.6574675324675324, Test Acc = 0.6447368421052632\n",
    "# Epoch = 161, loss = 42.02638882398605\n",
    "# Epoch = 162, loss = 42.76215770840645\n",
    "# Epoch = 163, loss = 41.943300515413284\n",
    "# Epoch = 164, loss = 41.744627594947815\n",
    "# Epoch = 165, loss = 41.64369750022888\n",
    "# Epoch = 166, loss = 41.82700064778328\n",
    "# Epoch = 167, loss = 41.838323920965195\n",
    "# Epoch = 168, loss = 42.99936231970787\n",
    "# Epoch = 169, loss = 42.55475810170174\n",
    "# Epoch = 170, loss = 42.43961876630783\n",
    "# j = 321, Train Acc = 0.9050324675324676, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.724025974025974, Test Acc = 0.7236842105263158\n",
    "# j = 200, Train Acc = 0.6525974025974026, Test Acc = 0.6513157894736842\n",
    "# Epoch = 171, loss = 42.14726781845093\n",
    "# Epoch = 172, loss = 42.02729895710945\n",
    "# Epoch = 173, loss = 42.05796667933464\n",
    "# Epoch = 174, loss = 42.047207325696945\n",
    "# Epoch = 175, loss = 41.85863038897514\n",
    "# Epoch = 176, loss = 41.78750139474869\n",
    "# Epoch = 177, loss = 41.89787817001343\n",
    "# Epoch = 178, loss = 41.723336696624756\n",
    "# Epoch = 179, loss = 41.46990704536438\n",
    "# Epoch = 180, loss = 41.31174489855766\n",
    "# j = 321, Train Acc = 0.9147727272727273, Test Acc = 0.9539473684210527\n",
    "# j = 250, Train Acc = 0.7386363636363636, Test Acc = 0.743421052631579\n",
    "# j = 200, Train Acc = 0.6728896103896104, Test Acc = 0.6546052631578947\n",
    "# Epoch = 181, loss = 41.147144973278046\n",
    "# Epoch = 182, loss = 41.04915153980255\n",
    "# Epoch = 183, loss = 41.02901268005371\n",
    "# Epoch = 184, loss = 41.16104966402054\n",
    "# Epoch = 185, loss = 41.052144914865494\n",
    "# Epoch = 186, loss = 40.75493836402893\n",
    "# Epoch = 187, loss = 40.8061888217926\n",
    "# Epoch = 188, loss = 41.45923164486885\n",
    "# Epoch = 189, loss = 40.893089562654495\n",
    "# Epoch = 190, loss = 40.89757114648819\n",
    "# j = 321, Train Acc = 0.9188311688311688, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.734577922077922, Test Acc = 0.7138157894736842\n",
    "# j = 200, Train Acc = 0.6728896103896104, Test Acc = 0.6447368421052632\n",
    "# Epoch = 191, loss = 40.91097801923752\n",
    "# Epoch = 192, loss = 40.533461928367615\n",
    "# Epoch = 193, loss = 40.0526128411293\n",
    "# Epoch = 194, loss = 40.267620861530304\n",
    "# Epoch = 195, loss = 40.43658027052879\n",
    "# Epoch = 196, loss = 39.8879688680172\n",
    "# Epoch = 197, loss = 40.29349622130394\n",
    "# Epoch = 198, loss = 39.77166989445686\n",
    "# Epoch = 199, loss = 40.42837768793106\n",
    "# Epoch = 200, loss = 39.85610529780388\n",
    "# j = 321, Train Acc = 0.9066558441558441, Test Acc = 0.9506578947368421\n",
    "# j = 250, Train Acc = 0.7564935064935064, Test Acc = 0.7105263157894737\n",
    "# j = 200, Train Acc = 0.698051948051948, Test Acc = 0.6381578947368421"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
